{"cells":[{"source":"# Reshaping data with pandas\n","metadata":{},"cell_type":"markdown","id":"29c1f9f6-e81b-4ac5-814e-e0da865ebc84"},{"source":"This notes were taken from the excellent course \"Reshaping data with pandas\" in DataCamp.  \nIt includes from the concepts of wide and large formats of dataframes and how convert from one to another with distinc functions of pandas. It also covers how to manipulate json documents to convert them in dataframes.  \n\nExcepting for the json files, the datasets are in the file \"fifa_players.xlsx\" (which is here in the same repository of my github), in the different 17 sheets. The data for the exercises are no avaliable to download so I had to compile it by hand in spreadsheets.","metadata":{},"cell_type":"markdown","id":"c35c8bb4-0393-4154-bae5-b6b45b5eed9b"},{"source":"## Wide and large formats\nShape refers to the way in which data is organized in rows and columns.  \nA dataset is commonly organized in rows and columns.  \n\nThe wide format has no repeated records, but this could lead to missing values. This format is preferred to do simple statistics, such as calculating the mean, or imputing missing values.  \n\nThe long format is usually seen as the standard for a tidy dataset. There are multiple rows, for one for each feature.  \n\nIn a broad sense, reshaping data is transforming a data structure to adjust it for our analysis. This could involve something as simple as transposing the data so columns become rows and rows become columns.  \n\nWe define reshaping data as converting data from wide to long format an viceversa.  \n\nTo transform from wide to long format we use the pandas methods .melt() and .wide_to_long()  \n\nTo transform from long to wide format we use the pandas methods .pivot() and .pivot_table()","metadata":{},"cell_type":"markdown","id":"759fa080-eed7-426f-a1a1-278546b023e6"},{"source":"import pandas as pd\nfifa_players=pd.DataFrame({\"name\":[\"Lionel Messi\",\"Cristiano Ronald\",\"Neymar da Silva\",\"Jan Oblak\",\"Eden Hazard\"],\"age\":[32,34,27,26,28],\"height\":[170,187,175,188,175],\"weight\":[72,83,68,87,74],\"nationality\":[\"Argentina\",\"Portugal\",\"Brazil\",\"Slovenia\",\"Belgium\"],\"club\":[\"FC Barcelona\",\"Juventus\",\"Paris Saint-Germain\",\"Atlético Madrid\",\"Real Madrid\"]})","metadata":{"id":"bA5ajAmk7XH6","executionTime":88,"lastSuccessfullyExecutedCode":"import pandas as pd\nfifa_players=pd.DataFrame({\"name\":[\"Lionel Messi\",\"Cristiano Ronald\",\"Neymar da Silva\",\"Jan Oblak\",\"Eden Hazard\"],\"age\":[32,34,27,26,28],\"height\":[170,187,175,188,175],\"weight\":[72,83,68,87,74],\"nationality\":[\"Argentina\",\"Portugal\",\"Brazil\",\"Slovenia\",\"Belgium\"],\"club\":[\"FC Barcelona\",\"Juventus\",\"Paris Saint-Germain\",\"Atlético Madrid\",\"Real Madrid\"]})"},"id":"d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7","cell_type":"code","execution_count":1,"outputs":[]},{"source":"Explore your dataset and reshape it using basic steps, such as setting different indices, filtering columns and flipping the DataFrame.","metadata":{},"cell_type":"markdown","id":"48d78fb0-2322-46f0-8ca4-dce5c951adc2"},{"source":"print(fifa_players)","metadata":{"executionTime":119,"lastSuccessfullyExecutedCode":"print(fifa_players)"},"cell_type":"code","id":"e976b63e-1152-4b62-a16c-d98df05b4cbc","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"               name  age  height  weight nationality                 club\n0      Lionel Messi   32     170      72   Argentina         FC Barcelona\n1  Cristiano Ronald   34     187      83    Portugal             Juventus\n2   Neymar da Silva   27     175      68      Brazil  Paris Saint-Germain\n3         Jan Oblak   26     188      87    Slovenia      Atlético Madrid\n4       Eden Hazard   28     175      74     Belgium          Real Madrid\n"}]},{"source":"# Set name as index\nfifa_transpose = fifa_players.set_index(\"name\")\n\n# Print fifa_transpose\nprint(fifa_transpose)","metadata":{"executionTime":114,"lastSuccessfullyExecutedCode":"# Set name as index\nfifa_transpose = fifa_players.set_index(\"name\")\n\n# Print fifa_transpose\nprint(fifa_transpose)"},"cell_type":"code","id":"ec2fc814-6eee-47f1-8013-53c7abf30980","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"                  age  height  weight nationality                 club\nname                                                                  \nLionel Messi       32     170      72   Argentina         FC Barcelona\nCristiano Ronald   34     187      83    Portugal             Juventus\nNeymar da Silva    27     175      68      Brazil  Paris Saint-Germain\nJan Oblak          26     188      87    Slovenia      Atlético Madrid\nEden Hazard        28     175      74     Belgium          Real Madrid\n"}]},{"source":"# Modify the DataFrame to keep only height and weight columns\nfifa_transpose = fifa_players.set_index('name')[['height','weight']]\n\n# Print fifa_transpose\nprint(fifa_transpose)","metadata":{"executionTime":114,"lastSuccessfullyExecutedCode":"# Modify the DataFrame to keep only height and weight columns\nfifa_transpose = fifa_players.set_index('name')[['height','weight']]\n\n# Print fifa_transpose\nprint(fifa_transpose)"},"cell_type":"code","id":"dd28bc89-9a48-4dcd-b98b-0a283fc53b78","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"                  height  weight\nname                            \nLionel Messi         170      72\nCristiano Ronald     187      83\nNeymar da Silva      175      68\nJan Oblak            188      87\nEden Hazard          175      74\n"}]},{"source":"# Change the DataFrame so rows become columns and vice versa\nfifa_transpose = fifa_players.set_index('name')[['height', 'weight']].transpose()\n\n# Print fifa_transpose\nprint(fifa_transpose)","metadata":{"executionTime":72,"lastSuccessfullyExecutedCode":"# Change the DataFrame so rows become columns and vice versa\nfifa_transpose = fifa_players.set_index('name')[['height', 'weight']].transpose()\n\n# Print fifa_transpose\nprint(fifa_transpose)"},"cell_type":"code","id":"fb070e0c-6b7d-47e8-9184-562d0dfc3753","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"name    Lionel Messi  Cristiano Ronald  Neymar da Silva  Jan Oblak  Eden Hazard\nheight           170               187              175        188          175\nweight            72                83               68         87           74\n"}]},{"source":"## Reshaping using the pivot method\nThe long format is usually the most suitable to store and clean a dataset.  \n\nThe pivot method allows us to reshape the data from a long to wide format. It takes three arguments  \ndf.pivot(index=,columns=,values=)  \nBy ommitting the values argument we extend the pivot method to all the column values.  \n\nPassing only index and columns arguments to the pivot method will work in most of the cases.  ","metadata":{},"cell_type":"markdown","id":"77179af9-8dba-4740-9008-03e1d27af39d"},{"source":"import pandas as pd\nfifa_players=pd.DataFrame({\"name\":[\"L. Messi\",\"Cristiano Ronaldo\",\"L. Messi\",\"Cristiano Ronaldo\",\"L. Messi\",\"Cristiano Ronaldo\"],\"movement\":[\"shooting\",\"shooting\",\"passing\",\"passing\",\"dribbling\",\"dribbling\"],\"overall\":[93,92,92,82,96,89],\"attacking\":[70,89,92,83,88,84]})","metadata":{"executionTime":134,"lastSuccessfullyExecutedCode":"import pandas as pd\nfifa_players=pd.DataFrame({\"name\":[\"L. Messi\",\"Cristiano Ronaldo\",\"L. Messi\",\"Cristiano Ronaldo\",\"L. Messi\",\"Cristiano Ronaldo\"],\"movement\":[\"shooting\",\"shooting\",\"passing\",\"passing\",\"dribbling\",\"dribbling\"],\"overall\":[93,92,92,82,96,89],\"attacking\":[70,89,92,83,88,84]})"},"cell_type":"code","id":"c5458d7e-cd36-4b21-9659-46a9e26ea618","execution_count":22,"outputs":[]},{"source":"print(fifa_players)","metadata":{"executionTime":72,"lastSuccessfullyExecutedCode":"print(fifa_players)"},"cell_type":"code","id":"2731b936-4f0a-4c63-9fb7-87997a97e4ff","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":"                name   movement  overall  attacking\n0           L. Messi   shooting       93         70\n1  Cristiano Ronaldo   shooting       92         89\n2           L. Messi    passing       92         92\n3  Cristiano Ronaldo    passing       82         83\n4           L. Messi  dribbling       96         88\n5  Cristiano Ronaldo  dribbling       89         84\n"}]},{"source":"# Pivot fifa_players to get overall scores indexed by name and identified by movement\nfifa_overall = fifa_players.pivot(index=\"name\", columns=\"movement\", values=\"overall\")\n\n# Print fifa_overall\nprint(fifa_overall)","metadata":{"executionTime":67,"lastSuccessfullyExecutedCode":"# Pivot fifa_players to get overall scores indexed by name and identified by movement\nfifa_overall = fifa_players.pivot(index=\"name\", columns=\"movement\", values=\"overall\")\n\n# Print fifa_overall\nprint(fifa_overall)"},"cell_type":"code","id":"2ae4ad25-e2e8-461c-85c9-fc9ae94de47e","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"movement           dribbling  passing  shooting\nname                                           \nCristiano Ronaldo         89       82        92\nL. Messi                  96       92        93\n"}]},{"source":"# Pivot fifa_players to get attacking scores indexed by name and identified by movement\nfifa_attacking = fifa_players.pivot(index=\"name\", columns=\"movement\", values=\"attacking\")\n\n# Print fifa_attacking\nprint(fifa_attacking)","metadata":{"executionTime":71,"lastSuccessfullyExecutedCode":"# Pivot fifa_players to get attacking scores indexed by name and identified by movement\nfifa_attacking = fifa_players.pivot(index=\"name\", columns=\"movement\", values=\"attacking\")\n\n# Print fifa_attacking\nprint(fifa_attacking)"},"cell_type":"code","id":"e909b9d6-ae9d-4f4c-8ded-3a3dbad482a7","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"movement           dribbling  passing  shooting\nname                                           \nCristiano Ronaldo         84       83        89\nL. Messi                  88       92        70\n"}]},{"source":"# Use the pivot method to get overall scores indexed by movement and identified by name\nfifa_names = fifa_players.pivot(index=\"movement\",columns=\"name\",values=\"overall\")\n\n# Print fifa_names\nprint(fifa_names)","metadata":{"executionTime":63,"lastSuccessfullyExecutedCode":"# Use the pivot method to get overall scores indexed by movement and identified by name\nfifa_names = fifa_players.pivot(index=\"movement\",columns=\"name\",values=\"overall\")\n\n# Print fifa_names\nprint(fifa_names)"},"cell_type":"code","id":"e7e8c856-1f41-418c-929d-0397bf7c9f68","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":"name       Cristiano Ronaldo  L. Messi\nmovement                              \ndribbling                 89        96\npassing                   82        92\nshooting                  92        93\n"}]},{"source":"# Pivot fifa_players to get overall and attacking scores indexed by name and identified by movement\nfifa_over_attack = fifa_players.pivot(index=\"name\", \n                                     columns=\"movement\", \n                                     values=[\"overall\", \"attacking\"])\n\n# Print fifa_over_attack\nprint(fifa_over_attack)","metadata":{"executionTime":175,"lastSuccessfullyExecutedCode":"# Pivot fifa_players to get overall and attacking scores indexed by name and identified by movement\nfifa_over_attack = fifa_players.pivot(index=\"name\", \n                                     columns=\"movement\", \n                                     values=[\"overall\", \"attacking\"])\n\n# Print fifa_over_attack\nprint(fifa_over_attack)"},"cell_type":"code","id":"65c904ef-d337-4954-87fc-550798e7cd32","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"                    overall                  attacking                 \nmovement          dribbling passing shooting dribbling passing shooting\nname                                                                   \nCristiano Ronaldo        89      82       92        84      83       89\nL. Messi                 96      92       93        88      92       70\n"}]},{"source":"# Use pivot method to get all the scores index by name and identified by movement\nfifa_all = fifa_players.pivot(index=\"name\",columns=\"movement\")\n\n# Print fifa_over_attack\nprint(fifa_all)","metadata":{"executionTime":149,"lastSuccessfullyExecutedCode":"# Use pivot method to get all the scores index by name and identified by movement\nfifa_all = fifa_players.pivot(index=\"name\",columns=\"movement\")\n\n# Print fifa_over_attack\nprint(fifa_all)"},"cell_type":"code","id":"b1f54a90-47a6-4e4f-8221-0cb9ace7bdb9","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"                    overall                  attacking                 \nmovement          dribbling passing shooting dribbling passing shooting\nname                                                                   \nCristiano Ronaldo        89      82       92        84      83       89\nL. Messi                 96      92       93        88      92       70\n"}]},{"source":"You suspect that there are different scores for the same index and column values. You remember that using the .pivot() method for all the columns does not work in that case. ","metadata":{},"cell_type":"markdown","id":"03c26d25-da02-4e90-a6cc-7347feceb58d"},{"source":"# Drop the fifth row to delete all repeated rows\nfifa_no_rep = fifa_players.drop(4, axis=0)\n\n# Pivot fifa players to get all scores by name and movement\nfifa_pivot = fifa_no_rep.pivot(index=\"name\",columns=\"movement\") \n\n# Print fifa_pivot\nprint(fifa_pivot)  ","metadata":{"executionTime":155,"lastSuccessfullyExecutedCode":"# Drop the fifth row to delete all repeated rows\nfifa_no_rep = fifa_players.drop(4, axis=0)\n\n# Pivot fifa players to get all scores by name and movement\nfifa_pivot = fifa_no_rep.pivot(index=\"name\",columns=\"movement\") \n\n# Print fifa_pivot\nprint(fifa_pivot)  "},"cell_type":"code","id":"219766d5-d539-41c1-b9be-459bfa85a2d2","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"                    overall                  attacking                 \nmovement          dribbling passing shooting dribbling passing shooting\nname                                                                   \nCristiano Ronaldo      89.0    82.0     92.0      84.0    83.0     89.0\nL. Messi                NaN    92.0     93.0       NaN    92.0     70.0\n"}]},{"source":"## Pivot table\nNow we are going the use .pivot_table() method.  \n\nThe simple pivot method has the limitation that the index/column pair must be unique. It can not aggregate values.  \n\nA pivot table is a data frame that contains statistics that summarizes the data of a larger data frame.  \n\nThis method takes several arguments  \ndf.pivot_table(index=,columns=,values=,aggfunc=\"\",margins=True/False)  \nPivot tables can have multileves indexes.  \n\nThen, when:\nThe dataframe have more than one value for each index/column pair  \nDo you need to have a multi-index in your resulting pivoted data frame  \nYou need summary statistics for a large data frame  \nUse .pivot_table()!","metadata":{},"cell_type":"markdown","id":"a4b10347-2671-4090-9120-ac843467dc7f"},{"source":"# Discard the fifth row to delete all repeated rows\nfifa_drop = fifa_players.drop(4,axis=0)\n\n# Use pivot method to get all scores by name and movement\nfifa_pivot = fifa_drop.pivot(index=\"name\",columns=\"movement\") \n\n# Print fifa_pivot\nprint(fifa_pivot)  \n\n# Use pivot table to get all scores by name and movement\nfifa_pivot_table = fifa_players.pivot_table(index=\"name\", \n                                     columns=\"movement\", \n                                     aggfunc=\"mean\")\n# Print fifa_pivot_table\nprint(fifa_pivot_table)","metadata":{"executionTime":147,"lastSuccessfullyExecutedCode":"# Discard the fifth row to delete all repeated rows\nfifa_drop = fifa_players.drop(4,axis=0)\n\n# Use pivot method to get all scores by name and movement\nfifa_pivot = fifa_drop.pivot(index=\"name\",columns=\"movement\") \n\n# Print fifa_pivot\nprint(fifa_pivot)  \n\n# Use pivot table to get all scores by name and movement\nfifa_pivot_table = fifa_players.pivot_table(index=\"name\", \n                                     columns=\"movement\", \n                                     aggfunc=\"mean\")\n# Print fifa_pivot_table\nprint(fifa_pivot_table)"},"cell_type":"code","id":"62535b1d-acd4-4912-a888-a2e97441f642","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"                    overall                  attacking                 \nmovement          dribbling passing shooting dribbling passing shooting\nname                                                                   \nCristiano Ronaldo      89.0    82.0     92.0      84.0    83.0     89.0\nL. Messi                NaN    92.0     93.0       NaN    92.0     70.0\n                  attacking                    overall                 \nmovement          dribbling passing shooting dribbling passing shooting\nname                                                                   \nCristiano Ronaldo        84      83       89        89      82       92\nL. Messi                 88      92       70        96      92       93\n"}]},{"source":"Now, it's time to continue working on the fifa_players exploration. Your next task is to examine the characteristics of players belonging to different teams.\n\nParticularly, you are interested in players from two big rival teams: Barcelona and Real Madrid.\n\nYou decide that .pivot_table() is the best tool to get your results since it's an easy way to generate a report. Also, it allows you to define aggregation functions and work with multiple indices.","metadata":{},"cell_type":"markdown","id":"ce9db3d5-226e-4c9c-9cc6-7abb931c47c6"},{"source":"import pandas as pd\nimport openpyxl\nfifa_players=pd.read_excel(\"fifa_players.xlsx\")","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport openpyxl\nfifa_players=pd.read_excel(\"fifa_players.xlsx\")"},"cell_type":"code","id":"4337cd2d-b33c-4f24-9c74-0098c6b99909","execution_count":14,"outputs":[]},{"source":"# Use pivot table to display mean age of players by club and nationality \nmean_age_fifa = fifa_players.pivot_table(index=\"nationality\", \n                                  columns=['club'], \n                                  values='age', \n                                  aggfunc='mean')\n\n# Print mean_age_fifa\nprint(mean_age_fifa)","metadata":{"executionTime":151,"lastSuccessfullyExecutedCode":"# Use pivot table to display mean age of players by club and nationality \nmean_age_fifa = fifa_players.pivot_table(index=\"nationality\", \n                                  columns=['club'], \n                                  values='age', \n                                  aggfunc='mean')\n\n# Print mean_age_fifa\nprint(mean_age_fifa)"},"cell_type":"code","id":"d2bd342b-edfd-4000-9a9e-372633cd3ef3","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":"club         FC Barcelona\nnationality              \nBrazil          24.222222\nCroatia         32.000000\nFrance          24.875000\nGermany         28.000000\nUruguay         26.000000\n"}]},{"source":"# Use pivot table to display max height of any player by club and nationality\ntall_players_fifa = fifa_players.pivot_table(index=\"nationality\", \n                                     columns=\"club\", \n                                      values=\"height\", \n                                      aggfunc=\"max\")\n\n# Print tall_players_fifa\nprint(tall_players_fifa)","metadata":{"executionTime":146,"lastSuccessfullyExecutedCode":"# Use pivot table to display max height of any player by club and nationality\ntall_players_fifa = fifa_players.pivot_table(index=\"nationality\", \n                                     columns=\"club\", \n                                      values=\"height\", \n                                      aggfunc=\"max\")\n\n# Print tall_players_fifa\nprint(tall_players_fifa)"},"cell_type":"code","id":"651ee2a8-b2a8-49d8-8234-e62d4d3d1d32","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":"club         FC Barcelona\nnationality              \nBrazil                190\nCroatia               184\nFrance                191\nGermany               187\nUruguay               182\n"}]},{"source":"# Use pivot table to show the count of players by club and nationality and the total count\nplayers_country = fifa_players.pivot_table(index=\"nationality\", \n                                    columns=\"club\", \n                                    values=\"name\", \n                                    aggfunc=\"count\", \n                                    margins=True)\n\n# Print players_country\nprint(players_country)","metadata":{"executionTime":151,"lastSuccessfullyExecutedCode":"# Use pivot table to show the count of players by club and nationality and the total count\nplayers_country = fifa_players.pivot_table(index=\"nationality\", \n                                    columns=\"club\", \n                                    values=\"name\", \n                                    aggfunc=\"count\", \n                                    margins=True)\n\n# Print players_country\nprint(players_country)"},"cell_type":"code","id":"0e580e1f-ed5f-4f90-8393-d449b5eed82f","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":"club         FC Barcelona  All\nnationality                   \nBrazil                  9    9\nCroatia                 2    2\nFrance                  8    8\nGermany                 2    2\nUruguay                 2    2\nAll                    23   23\n"}]},{"source":"You will continue your exploration of characteristics of players in fifa_players belonging to two teams: FC Barcelona and Real Madrid. As your last task, you are interested in exploring the maximum height and weight separated by teams and nationality. You will also compare two years, 2000 and 2010.\n\nYou have two columns that you want to set as an index, so you will need to use pivot_table().\n\nThe fifa_players dataset is available for you. It contains data about the club, nationality, height, weight, and year of the players playing for each team.","metadata":{},"cell_type":"markdown","id":"ea46a4a2-a7e9-4b05-b1bc-d8143f0c71d2"},{"source":"import pandas as pd\nfifa_players=pd.read_excel(\"fifa_players.xlsx\",'Sheet2')\nprint(fifa_players)","metadata":{"executionTime":132,"lastSuccessfullyExecutedCode":"import pandas as pd\nfifa_players=pd.read_excel(\"fifa_players.xlsx\",'Sheet2')\nprint(fifa_players)"},"cell_type":"code","id":"2b76b8b8-d432-46db-8b05-d0eab9619699","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":"           club nationality  year  height  weight\n0  FC Barcelona     Germany  2000     187      85\n1  FC Barcelona     Germany  2010     189      87\n2   Real Madrid     Croatia  2000     172      66\n3   Real Madrid     Croatia  2010     173      68\n4   Real Madrid     Germany  2000     183      76\n5   Real Madrid     Germany  2010     185      77\n6  FC Barcelona     Croatia  2000     184      78\n7  FC Barcelona     Croatia  2010     185      76\n"}]},{"source":"# Define a pivot table to get the characteristic by nationality and club\nfifa_mean = fifa_players.pivot_table(index=[\"nationality\",\"club\"],columns=\"year\")\n\n# Print fifa_mean\nprint(fifa_mean)","metadata":{"executionTime":140,"lastSuccessfullyExecutedCode":"# Define a pivot table to get the characteristic by nationality and club\nfifa_mean = fifa_players.pivot_table(index=[\"nationality\",\"club\"],columns=\"year\")\n\n# Print fifa_mean\nprint(fifa_mean)"},"cell_type":"code","id":"590c441f-f533-46e8-a894-bcffb33c5f78","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":"                         height      weight     \nyear                       2000 2010   2000 2010\nnationality club                                \nCroatia     FC Barcelona    184  185     78   76\n            Real Madrid     172  173     66   68\nGermany     FC Barcelona    187  189     85   87\n            Real Madrid     183  185     76   77\n"}]},{"source":"# Set the appropriate argument to show the maximum values\nfifa_mean = fifa_players.pivot_table(index=['nationality', 'club'], \n                                     columns='year', \n                                     aggfunc=\"max\")\n\n# Print fifa_mean\nprint(fifa_mean)","metadata":{"executionTime":160,"lastSuccessfullyExecutedCode":"# Set the appropriate argument to show the maximum values\nfifa_mean = fifa_players.pivot_table(index=['nationality', 'club'], \n                                     columns='year', \n                                     aggfunc=\"max\")\n\n# Print fifa_mean\nprint(fifa_mean)"},"cell_type":"code","id":"fe7aa4a9-aa86-4e20-86dc-fcd13cb59451","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":"                         height      weight     \nyear                       2000 2010   2000 2010\nnationality club                                \nCroatia     FC Barcelona    184  185     78   76\n            Real Madrid     172  173     66   68\nGermany     FC Barcelona    187  189     85   87\n            Real Madrid     183  185     76   77\n"}]},{"source":"# Set the argument to get the maximum for each row and column\nfifa_mean = fifa_players.pivot_table(index=['nationality', 'club'], \n                                     columns='year', \n                                     aggfunc='max', \n                                     margins=True)\n\n# Print fifa_mean\nprint(fifa_mean)","metadata":{"executionTime":154,"lastSuccessfullyExecutedCode":"# Set the argument to get the maximum for each row and column\nfifa_mean = fifa_players.pivot_table(index=['nationality', 'club'], \n                                     columns='year', \n                                     aggfunc='max', \n                                     margins=True)\n\n# Print fifa_mean\nprint(fifa_mean)"},"cell_type":"code","id":"2320a0a4-e7d2-4329-8898-5a4e96e61174","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":"                         height           weight         \nyear                       2000 2010  All   2000 2010 All\nnationality club                                         \nCroatia     FC Barcelona    184  185  185     78   76  78\n            Real Madrid     172  173  173     66   68  68\nGermany     FC Barcelona    187  189  189     85   87  87\n            Real Madrid     183  185  185     76   77  77\nAll                         187  189  189     85   87  87\n"}]},{"source":"## Reshaping with melt\nWe want to perform from wide to long.    \nFor some tasks to perform analytics or plot graphs we need the data in the long format.  \n\nPandas provides the melt function with the arguments  \ndf.melt(id_vars=[\"\",\"\"],value_vars=,var_name=\"var_we_analyze\",value_name=)  \nIn var_name the columns which we want to melt.    ","metadata":{},"cell_type":"markdown","id":"68e008ae-4911-444c-b02a-386fcfbcbb09"},{"source":"import pandas as pd\nbooks_gothic=pd.read_excel(\"fifa_players.xlsx\",\"Sheet3\")","metadata":{"executionTime":146,"lastSuccessfullyExecutedCode":"import pandas as pd\nbooks_gothic=pd.read_excel(\"fifa_players.xlsx\",\"Sheet3\")"},"cell_type":"code","id":"bb2c3607-05cf-433d-a287-ba92b32f889b","execution_count":27,"outputs":[]},{"source":"# Melt books_gothic using the title column as identifier \ngothic_melted = books_gothic.melt(id_vars=\"title\")\n\n# Print gothic_melted\nprint(gothic_melted)","metadata":{"executionTime":148,"lastSuccessfullyExecutedCode":"# Melt books_gothic using the title column as identifier \ngothic_melted = books_gothic.melt(id_vars=\"title\")\n\n# Print gothic_melted\nprint(gothic_melted)"},"cell_type":"code","id":"f54742aa-f100-4b21-91e4-08304ccfdd2d","execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":"                         title      variable              value\n0            Wuthering Heights       authors       Emily Bronte\n1                Frankeinstein       authors       Mary Shelley\n2   The Picture of Dorian Gray       authors        Oscar Wilde\n3            Wuthering Heights     num_pages                322\n4                Frankeinstein     num_pages                189\n5   The Picture of Dorian Gray     num_pages                187\n6            Wuthering Heights  rating_count       rating_count\n7                Frankeinstein  rating_count               2452\n8   The Picture of Dorian Gray  rating_count               3342\n9            Wuthering Heights        rating               3.85\n10               Frankeinstein        rating               4.31\n11  The Picture of Dorian Gray        rating               4.15\n12           Wuthering Heights     publisher      Penguin Books\n13               Frankeinstein     publisher  Kaplan Publishing\n14  The Picture of Dorian Gray     publisher            Pearson\n"}]},{"source":"# Melt books_gothic using the title, authors, and publisher columns as identifier\ngothic_melted_new = books_gothic.melt(id_vars=['title','authors','publisher'])\n\n# Print gothic_melted_new\nprint(gothic_melted_new)","metadata":{"executionTime":157,"lastSuccessfullyExecutedCode":"# Melt books_gothic using the title, authors, and publisher columns as identifier\ngothic_melted_new = books_gothic.melt(id_vars=['title','authors','publisher'])\n\n# Print gothic_melted_new\nprint(gothic_melted_new)"},"cell_type":"code","id":"52c75cc5-57b7-4d1f-93a8-94b33f63d789","execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":"                        title       authors  ...      variable         value\n0           Wuthering Heights  Emily Bronte  ...     num_pages           322\n1               Frankeinstein  Mary Shelley  ...     num_pages           189\n2  The Picture of Dorian Gray   Oscar Wilde  ...     num_pages           187\n3           Wuthering Heights  Emily Bronte  ...  rating_count  rating_count\n4               Frankeinstein  Mary Shelley  ...  rating_count          2452\n5  The Picture of Dorian Gray   Oscar Wilde  ...  rating_count          3342\n6           Wuthering Heights  Emily Bronte  ...        rating          3.85\n7               Frankeinstein  Mary Shelley  ...        rating          4.31\n8  The Picture of Dorian Gray   Oscar Wilde  ...        rating          4.15\n\n[9 rows x 5 columns]\n"}]},{"source":"Your first exploration of the books_gothic dataset was successful. Now, your next task is to perform a more detailed analysis. You need to reshape your DataFrame again. This time, you don't want to use all of your variables.\n\nTo that aim, you will melt your DataFrame, taking several approaches using different columns as identifiers and value variables.\n\nThe same books_gothic dataset you used before is available for you. It contains data about the title, author, number_pages, rating, rating_count, and publisher of each book. Make sure to examine it in the console!","metadata":{},"cell_type":"markdown","id":"86a846fa-77eb-444f-9156-ccd24f056b15"},{"source":"# Melt publisher column using title and authors as identifiers\npublisher_melted = books_gothic.melt(id_vars=[\"title\",\"authors\"], \n                                     value_vars=\"publisher\")\n\n# Print publisher_melted\nprint(publisher_melted)","metadata":{"executionTime":169,"lastSuccessfullyExecutedCode":"# Melt publisher column using title and authors as identifiers\npublisher_melted = books_gothic.melt(id_vars=[\"title\",\"authors\"], \n                                     value_vars=\"publisher\")\n\n# Print publisher_melted\nprint(publisher_melted)"},"cell_type":"code","id":"8d75378a-6e20-4238-9099-39714f55e417","execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":"                        title       authors   variable              value\n0           Wuthering Heights  Emily Bronte  publisher      Penguin Books\n1               Frankeinstein  Mary Shelley  publisher  Kaplan Publishing\n2  The Picture of Dorian Gray   Oscar Wilde  publisher            Pearson\n"}]},{"source":"# Melt rating and rating_count columns using the title as identifier\nrating_melted = books_gothic.melt(id_vars=\"title\", \n                                  value_vars=[\"rating\",\"rating_count\"])\n\n# Print rating_melted\nprint(rating_melted)","metadata":{"executionTime":163,"lastSuccessfullyExecutedCode":"# Melt rating and rating_count columns using the title as identifier\nrating_melted = books_gothic.melt(id_vars=\"title\", \n                                  value_vars=[\"rating\",\"rating_count\"])\n\n# Print rating_melted\nprint(rating_melted)"},"cell_type":"code","id":"2eed7b66-22cc-488d-8124-85e929a26c09","execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":"                        title      variable         value\n0           Wuthering Heights        rating          3.85\n1               Frankeinstein        rating          4.31\n2  The Picture of Dorian Gray        rating          4.15\n3           Wuthering Heights  rating_count  rating_count\n4               Frankeinstein  rating_count          2452\n5  The Picture of Dorian Gray  rating_count          3342\n"}]},{"source":"# Melt rating and rating_count columns using title and authors as identifier\nbooks_melted = books_gothic.melt(id_vars=[\"title\",\"authors\"], \n                                 value_vars=[\"rating\",'rating_count'])\n\n# Print books_melted\nprint(books_melted)","metadata":{"executionTime":174,"lastSuccessfullyExecutedCode":"# Melt rating and rating_count columns using title and authors as identifier\nbooks_melted = books_gothic.melt(id_vars=[\"title\",\"authors\"], \n                                 value_vars=[\"rating\",'rating_count'])\n\n# Print books_melted\nprint(books_melted)"},"cell_type":"code","id":"b23b3bb7-297f-459b-839b-04f20bcd0054","execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":"                        title       authors      variable         value\n0           Wuthering Heights  Emily Bronte        rating          3.85\n1               Frankeinstein  Mary Shelley        rating          4.31\n2  The Picture of Dorian Gray   Oscar Wilde        rating          4.15\n3           Wuthering Heights  Emily Bronte  rating_count  rating_count\n4               Frankeinstein  Mary Shelley  rating_count          2452\n5  The Picture of Dorian Gray   Oscar Wilde  rating_count          3342\n"}]},{"source":"You are satisfied with the way you reshaped the books_gothic DataFrame, however, you would like to finish your work by naming the newly-created columns. This will help you clarify what the variables and values are.\n\nYou remember that .melt() allows you to do that. In order to achieve your goal, you will reshape your DataFrame in three steps.","metadata":{},"cell_type":"markdown","id":"e7b55460-48b1-498e-baf9-22c0d255e7a4"},{"source":"# Melt the rating and rating_count using title, authors and publisher as identifiers\nbooks_ratings = books_gothic.melt(id_vars=['title','authors','publisher'], \n                                  value_vars=[\"rating\",\"rating_count\"])\n\n# Print books_ratings\nprint(books_ratings)","metadata":{"executionTime":172,"lastSuccessfullyExecutedCode":"# Melt the rating and rating_count using title, authors and publisher as identifiers\nbooks_ratings = books_gothic.melt(id_vars=['title','authors','publisher'], \n                                  value_vars=[\"rating\",\"rating_count\"])\n\n# Print books_ratings\nprint(books_ratings)"},"cell_type":"code","id":"fad85b11-f57c-44f8-8e93-60f094fe57ea","execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":"                        title       authors  ...      variable         value\n0           Wuthering Heights  Emily Bronte  ...        rating          3.85\n1               Frankeinstein  Mary Shelley  ...        rating          4.31\n2  The Picture of Dorian Gray   Oscar Wilde  ...        rating          4.15\n3           Wuthering Heights  Emily Bronte  ...  rating_count  rating_count\n4               Frankeinstein  Mary Shelley  ...  rating_count          2452\n5  The Picture of Dorian Gray   Oscar Wilde  ...  rating_count          3342\n\n[6 rows x 5 columns]\n"}]},{"source":"# Assign the name number to the new column containing the values\nbooks_ratings = books_gothic.melt(id_vars=['title', 'authors', 'publisher'], \n                                  value_vars=['rating', 'rating_count'], \n                                  var_name='feature', \n                                  value_name=\"number\")\n\n# Print books_ratings\nprint(books_ratings)","metadata":{"executionTime":192,"lastSuccessfullyExecutedCode":"# Assign the name number to the new column containing the values\nbooks_ratings = books_gothic.melt(id_vars=['title', 'authors', 'publisher'], \n                                  value_vars=['rating', 'rating_count'], \n                                  var_name='feature', \n                                  value_name=\"number\")\n\n# Print books_ratings\nprint(books_ratings)"},"cell_type":"code","id":"03beed88-4488-4def-b68e-d3cc02f36e9a","execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":"                        title       authors  ...       feature        number\n0           Wuthering Heights  Emily Bronte  ...        rating          3.85\n1               Frankeinstein  Mary Shelley  ...        rating          4.31\n2  The Picture of Dorian Gray   Oscar Wilde  ...        rating          4.15\n3           Wuthering Heights  Emily Bronte  ...  rating_count  rating_count\n4               Frankeinstein  Mary Shelley  ...  rating_count          2452\n5  The Picture of Dorian Gray   Oscar Wilde  ...  rating_count          3342\n\n[6 rows x 5 columns]\n"}]},{"source":"## Wide to long function  \nWe can also use the wide_to_long() function.  \n \nThe format of this panda funtion is  \npd.wide_to_long(df,stubnames=[\"\",\"\"],i=,j=,sep='_',suffix='\\w+').  \n\ni is the index, and j tells pandas how we want to name the column that contains the suffix or the end of the wide columns. The suffix is needed when the name of the columns ends in a word.  ","metadata":{},"cell_type":"markdown","id":"ebf7ccc1-a28c-4117-8f18-c531fbdf7d66"},{"source":"import pandas as pd\ngolden_age=pd.read_excel(\"fifa_players.xlsx\",\"Sheet4\")","metadata":{"executionTime":188,"lastSuccessfullyExecutedCode":"import pandas as pd\ngolden_age=pd.read_excel(\"fifa_players.xlsx\",\"Sheet4\")"},"cell_type":"code","id":"45589738-9145-4193-a43a-1a0347af76a3","execution_count":2,"outputs":[]},{"source":"# Reshape wide to long using title as index and version as new name, and extracting isbn prefix \nisbn_long = pd.wide_to_long(golden_age, \n                    stubnames=\"isbn\", \n                    i=\"title\", \n                    j=\"version\")\n\n# Print isbn_long\nprint(isbn_long)","metadata":{"executionTime":185,"lastSuccessfullyExecutedCode":"# Reshape wide to long using title as index and version as new name, and extracting isbn prefix \nisbn_long = pd.wide_to_long(golden_age, \n                    stubnames=\"isbn\", \n                    i=\"title\", \n                    j=\"version\")\n\n# Print isbn_long\nprint(isbn_long)"},"cell_type":"code","id":"8a79f241-ef3d-4711-bcd5-6b85981e9cb3","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"                                       authors  ...           isbn\ntitle             version                       ...               \nThe Great Gatsby  13       F. Scott Fitzgerald  ...   978060098919\nThe Short Stories 13          Ernest Hemingway  ...  9780684837864\nTo the Lighthouse 13            Virginia Woolf  ...  9780156030472\nThe Great Gatsby  10       F. Scott Fitzgerald  ...     1572702567\nThe Short Stories 10          Ernest Hemingway  ...      684837862\nTo the Lighthouse 10            Virginia Woolf  ...      156030470\n\n[6 rows x 4 columns]\n"}]},{"source":"# Reshape wide to long using title and authors as index and version as new name, and prefix as wide column prefix\nprefix_long = pd.wide_to_long(golden_age, \n                      stubnames=\"prefix\", \n                      i=[\"title\", \"authors\"], \n                      j=\"version\")\n\n# Print prefix_long\nprint(prefix_long)","metadata":{"executionTime":171,"lastSuccessfullyExecutedCode":"# Reshape wide to long using title and authors as index and version as new name, and prefix as wide column prefix\nprefix_long = pd.wide_to_long(golden_age, \n                      stubnames=\"prefix\", \n                      i=[\"title\", \"authors\"], \n                      j=\"version\")\n\n# Print prefix_long\nprint(prefix_long)"},"cell_type":"code","id":"64280652-8e71-4391-b0cb-0c16736addb9","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"                                                      isbn13  ...  prefix\ntitle             authors             version                 ...        \nThe Great Gatsby  F. Scott Fitzgerald 13        978060098919  ...     978\n                                      10        978060098919  ...       1\nThe Short Stories Ernest Hemingway    13       9780684837864  ...     978\n                                      10       9780684837864  ...       0\nTo the Lighthouse Virginia Woolf      13       9780156030472  ...     978\n                                      10       9780156030472  ...       0\n\n[6 rows x 3 columns]\n"}]},{"source":"# Reshape wide to long using title and authors as index and version as new name, and prefix and isbn as wide column prefixes\nall_long = pd.wide_to_long(golden_age, \n                   stubnames=[\"isbn\",\"prefix\"], \n                   i=[\"title\",\"authors\"], \n                   j=\"version\")\n\n# Print all_long\nprint(all_long)","metadata":{"executionTime":142,"lastSuccessfullyExecutedCode":"# Reshape wide to long using title and authors as index and version as new name, and prefix and isbn as wide column prefixes\nall_long = pd.wide_to_long(golden_age, \n                   stubnames=[\"isbn\",\"prefix\"], \n                   i=[\"title\",\"authors\"], \n                   j=\"version\")\n\n# Print all_long\nprint(all_long)"},"cell_type":"code","id":"903d5383-eeee-443d-baa1-19941f8f4deb","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"                                                        isbn  prefix\ntitle             authors             version                       \nThe Great Gatsby  F. Scott Fitzgerald 13        978060098919     978\n                                      10          1572702567       1\nThe Short Stories Ernest Hemingway    13       9780684837864     978\n                                      10           684837862       0\nTo the Lighthouse Virginia Woolf      13       9780156030472     978\n                                      10           156030470       0\n"}]},{"source":"You explored the dataset books_brown and it needs reshaping. Again, you identified several columns to use as unique IDs and realized something different about the columns to group. Their name starts with a prefix, but this time, you identified a suffix and a separation element.\n\nThe books_brown dataset is available for you. It contains the title, author, and data about language_code, language_name, publisher_code, and publisher_name of each book. ","metadata":{},"cell_type":"markdown","id":"5887ac39-d13a-49c7-83ae-64aafa112062"},{"source":"import pandas as pd\nbooks_brown=pd.read_excel(\"fifa_players.xlsx\",\"Sheet5\")","metadata":{"executionTime":181,"lastSuccessfullyExecutedCode":"import pandas as pd\nbooks_brown=pd.read_excel(\"fifa_players.xlsx\",\"Sheet5\")"},"cell_type":"code","id":"398b7f62-1773-4138-a06a-3f3e91a72b8f","execution_count":8,"outputs":[]},{"source":"print(books_brown.head())","metadata":{"executionTime":149,"lastSuccessfullyExecutedCode":"print(books_brown.head())"},"cell_type":"code","id":"21399725-4e71-4085-9e65-e768627ed65e","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"                  title     author  ...  publisher_code publisher_name\n0     The Da Vinci Code  Dan Brown  ...              12   Random House\n1       Angels & Demons  Dan Brown  ...              34   Pocket Books\n2  La fortaleza digital  Dan Brown  ...              43        Umbriel\n\n[3 rows x 6 columns]\n"}]},{"source":"# Reshape using author and title as index, code as new name and getting the prefix language and publisher\nthe_code_long = pd.wide_to_long(books_brown, \n                                stubnames=[\"language\",\"publisher\"], \n                                j=\"code\", \n                                i=[\"author\",\"title\"])\n\n# Print the_code_long\nprint(the_code_long)","metadata":{"executionTime":193,"lastSuccessfullyExecutedCode":"# Reshape using author and title as index, code as new name and getting the prefix language and publisher\nthe_code_long = pd.wide_to_long(books_brown, \n                                stubnames=[\"language\",\"publisher\"], \n                                j=\"code\", \n                                i=[\"author\",\"title\"])\n\n# Print the_code_long\nprint(the_code_long)"},"cell_type":"code","id":"3cce15db-cf35-4ddf-92e2-65bf0254b638","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":"Empty DataFrame\nColumns: [language_code, publisher_name, language_name, publisher_code, language, publisher]\nIndex: []\n"}]},{"source":"# Specify underscore as the character that separates the variable names\nthe_code_long = pd.wide_to_long(books_brown, \n                                stubnames=['language', 'publisher'], \n                                i=['author', 'title'], \n                                j='code',sep=\"_\")\n\n# Print the_code_long\nprint(the_code_long)","metadata":{"executionTime":240,"lastSuccessfullyExecutedCode":"# Specify underscore as the character that separates the variable names\nthe_code_long = pd.wide_to_long(books_brown, \n                                stubnames=['language', 'publisher'], \n                                i=['author', 'title'], \n                                j='code',sep=\"_\")\n\n# Print the_code_long\nprint(the_code_long)"},"cell_type":"code","id":"c6faf1ce-06ea-4d6c-be65-1f15ace64f0b","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"Empty DataFrame\nColumns: [language_code, publisher_name, language_name, publisher_code, language, publisher]\nIndex: []\n"}]},{"source":"# Specify that wide columns have a suffix containing words\nthe_code_long = pd.wide_to_long(books_brown, \n                                stubnames=['language', 'publisher'], \n                                i=['author', 'title'], \n                                j='code', \n                                sep='_', \n                                suffix='\\w+')\n\n# Print the_code_long\nprint(the_code_long)","metadata":{"executionTime":209,"lastSuccessfullyExecutedCode":"# Specify that wide columns have a suffix containing words\nthe_code_long = pd.wide_to_long(books_brown, \n                                stubnames=['language', 'publisher'], \n                                i=['author', 'title'], \n                                j='code', \n                                sep='_', \n                                suffix='\\w+')\n\n# Print the_code_long\nprint(the_code_long)"},"cell_type":"code","id":"4ddb7b32-5f44-4692-9b5b-c3fa34137c7f","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"                                    language     publisher\nauthor    title                code                       \nDan Brown The Da Vinci Code    code        0            12\n                               name  english  Random House\n          Angels & Demons      code        0            34\n                               name  english  Pocket Books\n          La fortaleza digital code       84            43\n                               name  spanish       Umbriel\n"}]},{"source":"You would like to do an analysis for fun. You will analyze another book dataset, this time with the Hunger Games series.\n\nYou explored the dataset books_hunger before reshaping it, but something was not right. The index of the DataFrame contains the title of the books. You know that you cannot reshape it in this format. If you do, you will lose valuable data, the title, so you'll need to make some changes before transforming the DataFrame.\n\nThe books_hunger dataset is available for you. It contains the title, and data about language, publication date, publication number, and page number of each book.","metadata":{},"cell_type":"markdown","id":"035c60f0-209e-42a5-aaae-d44335c7f238"},{"source":"import pandas as pd\nbooks_hunger=pd.read_excel(\"fifa_players.xlsx\",\"Sheet6\")","metadata":{"executionTime":162,"lastSuccessfullyExecutedCode":"import pandas as pd\nbooks_hunger=pd.read_excel(\"fifa_players.xlsx\",\"Sheet6\")"},"cell_type":"code","id":"2a237a90-3ccc-46b0-b8be-395df87008ba","execution_count":1,"outputs":[]},{"source":"# Modify books_hunger by resetting the index without dropping it\nbooks_hunger.reset_index(drop=False, inplace=True)\n\n# Reshape using title and language as index, feature as new name, publication and page as prefix separated by space and ending in a word\npublication_features = pd.wide_to_long(books_hunger, \n                                       stubnames=[\"publication\",\"page\"], \n                                       i=[\"title\",\"language\"], \n                                       j=\"feature\", \n                                       sep=\" \", \n                                       suffix='\\w+')\n\n# Print publication_features\nprint(publication_features)","metadata":{"executionTime":230,"lastSuccessfullyExecutedCode":"# Modify books_hunger by resetting the index without dropping it\nbooks_hunger.reset_index(drop=False, inplace=True)\n\n# Reshape using title and language as index, feature as new name, publication and page as prefix separated by space and ending in a word\npublication_features = pd.wide_to_long(books_hunger, \n                                       stubnames=[\"publication\",\"page\"], \n                                       i=[\"title\",\"language\"], \n                                       j=\"feature\", \n                                       sep=\" \", \n                                       suffix='\\w+')\n\n# Print publication_features\nprint(publication_features)"},"cell_type":"code","id":"f0ce1261-0010-4668-a0e0-467157d38601","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"                                         index          publication   page\ntitle                  language feature                                   \nLos Juegos del Hambre  Spanish  date         0            5/25/2010    NaN\n                                number       0                    2  374.0\nCatching Fire          English  date         1            5/25/2012    NaN\n                                number       1                    6  391.0\nIl canto della rivolta Italian  date         2  2015-08-06 00:00:00    NaN\n                                number       2                    4  390.0\n"}]},{"source":"## Working with string columns\nWe are going to learn methods to reshape string columns.  \nIn pandas, we acces to the methods for strings through the str attribute.  \nTo split a column:  \ndf.str.split(\"separator\")  \n\nWe could also access one of the resulting elements. We use the get method from the str attributte\n.str.get(). We can use the expand argument with expand=True to create return a new data frame wit two columns, one for each split element.   \n\nThis allows to assign the split elements to columns in the original dataframe  \ndf[['newcol1','newcol2']]=df['col'].str.split(\"sep\",expand=True)  \nThis is useful because we can drop the original column, and after that transform the new dataframe  \ndf.drop('col',axis=1,inplace=True)  \npd.wide_to_long(df,stubnames=['',''],i=['newcol1','newcol2'],j=)\n","metadata":{},"cell_type":"markdown","id":"5f5c4ab2-d90d-4a12-b141-92e18b57d1ff"},{"source":"import pandas as pd\nbooks_dys=pd.read_excel(\"fifa_players.xlsx\",\"Sheet7\")\nauthor_list=['Ray Bradbury', 'George Orwell', 'Aldous Huxley']","metadata":{"executionTime":218,"lastSuccessfullyExecutedCode":"import pandas as pd\nbooks_dys=pd.read_excel(\"fifa_players.xlsx\",\"Sheet7\")\nauthor_list=['Ray Bradbury', 'George Orwell', 'Aldous Huxley']"},"cell_type":"code","id":"6f3e39dd-2a90-48bb-bf26-57e26d5f0bce","execution_count":15,"outputs":[]},{"source":"books_dys=books_dys.set_index(\"title\")","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"books_dys=books_dys.set_index(\"title\")"},"cell_type":"code","id":"05a33756-0926-471e-b8ee-32031c12019b","execution_count":16,"outputs":[]},{"source":"books_dys","metadata":{"executionTime":333,"lastSuccessfullyExecutedCode":"books_dys"},"cell_type":"code","id":"a12afaf4-06b5-4ce7-9623-86a5c7080adf","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"title","type":"string"},{"name":"year","type":"integer"},{"name":"num_pages","type":"integer"},{"name":"average_rating","type":"number"},{"name":"ratings_count","type":"integer"}],"primaryKey":["title"],"pandas_version":"1.4.0"},"data":[{"title":"Fahrenheit 451-1953","year":1953,"num_pages":186,"average_rating":4.1,"ratings_count":23244},{"title":"1984-1949","year":1949,"num_pages":268,"average_rating":4.31,"ratings_count":14353},{"title":"Brave New World-1932","year":1932,"num_pages":123,"average_rating":4.3,"ratings_count":23535}]},"total_rows":3,"truncation_type":null},"text/plain":"                      year  num_pages  average_rating  ratings_count\ntitle                                                               \nFahrenheit 451-1953   1953        186            4.10          23244\n1984-1949             1949        268            4.31          14353\nBrave New World-1932  1932        123            4.30          23535","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>num_pages</th>\n      <th>average_rating</th>\n      <th>ratings_count</th>\n    </tr>\n    <tr>\n      <th>title</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Fahrenheit 451-1953</th>\n      <td>1953</td>\n      <td>186</td>\n      <td>4.10</td>\n      <td>23244</td>\n    </tr>\n    <tr>\n      <th>1984-1949</th>\n      <td>1949</td>\n      <td>268</td>\n      <td>4.31</td>\n      <td>14353</td>\n    </tr>\n    <tr>\n      <th>Brave New World-1932</th>\n      <td>1932</td>\n      <td>123</td>\n      <td>4.30</td>\n      <td>23535</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"source":"You plan to work on a dataset about dystopian fiction books.\n\nBut first, you need to do some string manipulations. You realize that the DataFrame index contains data about the title and the release year. You can't find a column with the author of the book, so you decide to pre-define a list of the writers. Then, you want to delete the year and replace it with the author.\n\nYou decide that splitting the index and then concatenating it with the list is the best way to do it.\n\nThe books_dys dataset and author_list are available ","metadata":{},"cell_type":"markdown","id":"81bb40f3-ded3-466a-bc35-9b30e2d0dbc4"},{"source":"# Split the index of books_dys by the hyphen \nbooks_dys.index = books_dys.index.str.split(\"-\")\n\n# Print books_dys\nprint(books_dys)","metadata":{"executionTime":231,"lastSuccessfullyExecutedCode":"# Split the index of books_dys by the hyphen \nbooks_dys.index = books_dys.index.str.split(\"-\")\n\n# Print books_dys\nprint(books_dys)"},"cell_type":"code","id":"e8e6b055-ffb4-4943-8944-e5a7b16037ce","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"       year  num_pages  average_rating  ratings_count\ntitle                                                \nNaN    1953        186            4.10          23244\nNaN    1949        268            4.31          14353\nNaN    1932        123            4.30          23535\n"}]},{"source":"# Now that you've split the index, get the first element and assign it to the index of books_dys.\n# Get the first element after splitting the index of books_dys\nbooks_dys=pd.read_excel(\"fifa_players.xlsx\",\"Sheet7\")\nauthor_list=['Ray Bradbury', 'George Orwell', 'Aldous Huxley']\nbooks_dys=books_dys.set_index(\"title\")\n###\n###\n# Split by the hyphen the index of books_dys\nbooks_dys.index = books_dys.index.str.split('-').str.get(0)\n\n# Concatenate the index with the list author_list separated by a hyphen\nbooks_dys.index = books_dys.index.str.cat(author_list, sep=\"-\")\n\n# Print books_dys\nprint(books_dys)","metadata":{"executionTime":275,"lastSuccessfullyExecutedCode":"# Now that you've split the index, get the first element and assign it to the index of books_dys.\n# Get the first element after splitting the index of books_dys\nbooks_dys=pd.read_excel(\"fifa_players.xlsx\",\"Sheet7\")\nauthor_list=['Ray Bradbury', 'George Orwell', 'Aldous Huxley']\nbooks_dys=books_dys.set_index(\"title\")\n###\n###\n# Split by the hyphen the index of books_dys\nbooks_dys.index = books_dys.index.str.split('-').str.get(0)\n\n# Concatenate the index with the list author_list separated by a hyphen\nbooks_dys.index = books_dys.index.str.cat(author_list, sep=\"-\")\n\n# Print books_dys\nprint(books_dys)"},"cell_type":"code","id":"f178b1c0-e412-47a6-a919-f1b1262f277d","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":"                               year  num_pages  average_rating  ratings_count\ntitle                                                                        \nFahrenheit 451-Ray Bradbury    1953        186            4.10          23244\n1984-George Orwell             1949        268            4.31          14353\nBrave New World-Aldous Huxley  1932        123            4.30          23535\n"}]},{"source":"For your next task, you need to get an appropriate dataset containing ratings for all the Harry Potter books. You gathered data from Goodreads as well as from Amazon.\n\nYou realized that you need a long format, but the dataset hp_books is in a wide format. You want to melt the data, but first, you need to manipulate some of the string columns.\n\nThe full title is divided into two columns. The authors column contains info about the writer and the illustrator.\n\nRatings for the Harry Potter books are in the DataFrame hp_books","metadata":{},"cell_type":"markdown","id":"7441c451-1fae-4ed7-8088-c7c55ffe9bd7"},{"source":"import pandas as pd\nhp_books=pd.read_excel(\"fifa_players.xlsx\",\"Sheet8\")\nprint(hp_books)","metadata":{"executionTime":212,"lastSuccessfullyExecutedCode":"import pandas as pd\nhp_books=pd.read_excel(\"fifa_players.xlsx\",\"Sheet8\")\nprint(hp_books)"},"cell_type":"code","id":"21a50029-5d92-447d-87c0-4e4d734412ea","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"          title                  subtitle  ... goodreads  amazon\n0  Harry Potter     the Half-Blood Prince  ...      4.57    4.52\n1  Harry Potter  the Order of the Phoenix  ...      4.49    4.44\n2  Harry Potter    the Chamber of Secrets  ...      4.42    4.37\n3  Harry Potter  the Prisiones of Azkaban  ...      4.56    4.51\n4  Harry Potter       The Deathly Hallows  ...      4.42    4.37\n5  Harry Potter      the Sorcerer's Stone  ...      4.47    4.42\n6  Harry Potter        the Goblet of Fire  ...      4.56    4.51\n\n[7 rows x 5 columns]\n"}]},{"source":"# Concatenate the title and subtitle separated by \"and\" surrounded by spaces\nhp_books['full_title'] = hp_books['title'].str.cat(hp_books['subtitle'], sep =\" and \") \n\n# Split the authors into writer and illustrator columns\nhp_books[['writer', 'illustrator']] = hp_books['authors'].str.split('/', expand=True)\n\n# Melt goodreads and amazon columns into a single column\nhp_melt = hp_books.melt(id_vars=['full_title',\"writer\"], \n                        var_name=\"source\", \n                        value_vars=[\"goodreads\",\"amazon\"], \n                        value_name=\"rating\")\n\n# Print hp_melt\nprint(hp_melt)","metadata":{"executionTime":264,"lastSuccessfullyExecutedCode":"# Concatenate the title and subtitle separated by \"and\" surrounded by spaces\nhp_books['full_title'] = hp_books['title'].str.cat(hp_books['subtitle'], sep =\" and \") \n\n# Split the authors into writer and illustrator columns\nhp_books[['writer', 'illustrator']] = hp_books['authors'].str.split('/', expand=True)\n\n# Melt goodreads and amazon columns into a single column\nhp_melt = hp_books.melt(id_vars=['full_title',\"writer\"], \n                        var_name=\"source\", \n                        value_vars=[\"goodreads\",\"amazon\"], \n                        value_name=\"rating\")\n\n# Print hp_melt\nprint(hp_melt)"},"cell_type":"code","id":"bd11386a-5ef8-48cd-badb-2300b67f0bbc","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"                                   full_title        writer     source  rating\n0      Harry Potter and the Half-Blood Prince  J.K. Rowling  goodreads    4.57\n1   Harry Potter and the Order of the Phoenix  J.K. Rowling  goodreads    4.49\n2     Harry Potter and the Chamber of Secrets  J.K. Rowling  goodreads    4.42\n3   Harry Potter and the Prisiones of Azkaban  J.K. Rowling  goodreads    4.56\n4        Harry Potter and The Deathly Hallows  J.K. Rowling  goodreads    4.42\n5       Harry Potter and the Sorcerer's Stone  J.K. Rowling  goodreads    4.47\n6         Harry Potter and the Goblet of Fire  J.K. Rowling  goodreads    4.56\n7      Harry Potter and the Half-Blood Prince  J.K. Rowling     amazon    4.52\n8   Harry Potter and the Order of the Phoenix  J.K. Rowling     amazon    4.44\n9     Harry Potter and the Chamber of Secrets  J.K. Rowling     amazon    4.37\n10  Harry Potter and the Prisiones of Azkaban  J.K. Rowling     amazon    4.51\n11       Harry Potter and The Deathly Hallows  J.K. Rowling     amazon    4.37\n12      Harry Potter and the Sorcerer's Stone  J.K. Rowling     amazon    4.42\n13        Harry Potter and the Goblet of Fire  J.K. Rowling     amazon    4.51\n"}]},{"source":"Analyze data about Arthur Conan Doyle's books.\n\nYou realize your dataset, books_sh, needs reshaping. You notice there are columns that can be grouped using a prefix. You identify the columns to use as unique IDs. However, some of these columns contain strings. They need some manipulation before applying a wide to long transformation. You decide some of the strings need splitting to make the DataFrame cleaner.\n\nThe books_sh dataset is available for you. It contains the title, and data about version, number_pages, and number_ratings of each book.","metadata":{},"cell_type":"markdown","id":"f95ddef8-2b45-4d34-acfe-443f948332b5"},{"source":"import pandas as pd\nbooks_sh=pd.read_excel(\"fifa_players.xlsx\",\"Sheet9\")","metadata":{"executionTime":175,"lastSuccessfullyExecutedCode":"import pandas as pd\nbooks_sh=pd.read_excel(\"fifa_players.xlsx\",\"Sheet9\")"},"cell_type":"code","id":"4a75000f-d2a3-429b-8ea2-795bbad6ea36","execution_count":4,"outputs":[]},{"source":"# Split main_title by a colon and assign it to two columns named title and subtitle \nbooks_sh[['title', 'subtitle']] = books_sh['main_title'].str.split(':', expand=True)\n\n# Split version by a space and assign the second element to the column named volume\nbooks_sh['volume'] = books_sh['version'].str.split(' ').str.get(1)\n\n# Drop the main_title and version columns modifying books_sh\nbooks_sh.drop([\"main_title\", \"version\"], axis=1, inplace=True)\n\n# Print books_sh\nprint(books_sh)","metadata":{"executionTime":242,"lastSuccessfullyExecutedCode":"# Split main_title by a colon and assign it to two columns named title and subtitle \nbooks_sh[['title', 'subtitle']] = books_sh['main_title'].str.split(':', expand=True)\n\n# Split version by a space and assign the second element to the column named volume\nbooks_sh['volume'] = books_sh['version'].str.split(' ').str.get(1)\n\n# Drop the main_title and version columns modifying books_sh\nbooks_sh.drop([\"main_title\", \"version\"], axis=1, inplace=True)\n\n# Print books_sh\nprint(books_sh)"},"cell_type":"code","id":"c0334528-35a4-4516-9843-f14efce3cb05","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"   number_pages  number_ratings  ...              subtitle volume\n0          1059           24087  ...   The Complete Novels      I\n1           709           26794  ...   The Complete Novels     II\n2           334            2184  ...               Memoirs      I\n3           238            1884  ...               Memoirs     II\n\n[4 rows x 5 columns]\n"}]},{"source":"## Stacking dataframes\nPandas has methods designed to work with multi-level indexes dataframes.  \nA multiindex allows to store and manipulate multidimensional data in simple dataframes.  \n\nThere are several ways to create a multi-level index.  \nThe simplest one is with the .set_index(['',''],inplace=True)  \n\nAnother option is to use the method from_arrays().  \nWe define a list of lists  \nnew_array=[[],[]]  \nwhere each element represent one index. Then  \ndf.index=pd.MultiIndex.from_arrays(new_array,names=['',''])  \n\nWe could also define a DataFrame with multi-level indexes on the rows and the columns. The process is very similar.  \nindex=pd.MultiIndex.from_array([[],[]],names=)  \ncolumns=pd.MultiIndex.from_array([[],[]],names=)  \n\nnew_df=pd.DataFrame(data,index=index,columns=columns)  \n\nThe .stack() method will reshape the dataframe with a multi-level index by converting into a a stacked form. In other words, stacking means rearringing the innermost column index to become the innermost row index. It has the argument level= . If we don't set the level argument stack will move the last level by default.   ","metadata":{},"cell_type":"markdown","id":"6f3ce75f-1390-4bf2-a91f-e7c2d61ab113"},{"source":"Stack the calls!\nFirst, you explored the dataset churn and realized some information is missing. The dataset contains data about the total number of calls and the minutes spent on the phone by different customers. However, the state and city they live in are not listed.\n\nYou predefined an array with that data. You'd like to add it as an index in your DataFrame.\n\nThe DataFrame churn is available for you. It contains data about area code, total_day_calls and total_day_minutes.","metadata":{},"cell_type":"markdown","id":"fa7ee0b3-03d4-4333-b208-8f939038ed84"},{"source":"import pandas as pd\nchurn=pd.DataFrame({\"Area code\":[408,408,415,510],\"total_day_calls\":[116,109,84,67],\"total_day_minutes\":[204,287,84,50]})\nnew_index = [['California', 'California', 'New York', 'Ohio'], \n             ['Los Angeles', 'San Francisco', 'New York', 'Cleveland']]","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"import pandas as pd\nchurn=pd.DataFrame({\"Area code\":[408,408,415,510],\"total_day_calls\":[116,109,84,67],\"total_day_minutes\":[204,287,84,50]})\nnew_index = [['California', 'California', 'New York', 'Ohio'], \n             ['Los Angeles', 'San Francisco', 'New York', 'Cleveland']]"},"cell_type":"code","id":"11a632ad-0134-46b3-a262-648521acb5d9","execution_count":1,"outputs":[]},{"source":"churn","metadata":{"executionTime":274,"lastSuccessfullyExecutedCode":"churn"},"cell_type":"code","id":"ab53188d-13c3-48f2-81a5-f37a41fd08d0","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"Area code","type":"integer"},{"name":"total_day_calls","type":"integer"},{"name":"total_day_minutes","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"Area code":408,"total_day_calls":116,"total_day_minutes":204},{"index":1,"Area code":408,"total_day_calls":109,"total_day_minutes":287},{"index":2,"Area code":415,"total_day_calls":84,"total_day_minutes":84},{"index":3,"Area code":510,"total_day_calls":67,"total_day_minutes":50}]},"total_rows":4,"truncation_type":null},"text/plain":"   Area code  total_day_calls  total_day_minutes\n0        408              116                204\n1        408              109                287\n2        415               84                 84\n3        510               67                 50","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area code</th>\n      <th>total_day_calls</th>\n      <th>total_day_minutes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>408</td>\n      <td>116</td>\n      <td>204</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>408</td>\n      <td>109</td>\n      <td>287</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>415</td>\n      <td>84</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>510</td>\n      <td>67</td>\n      <td>50</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"source":"# Predefined list to use as index\nnew_index = [['California', 'California', 'New York', 'Ohio'], \n             ['Los Angeles', 'San Francisco', 'New York', 'Cleveland']]\n\n# Create a multi-level index using predefined new_index\nchurn_new = pd.MultiIndex.from_arrays(new_index, names=[\"state\", \"city\"])\n\n# Print churn_new\nprint(churn_new)","metadata":{"executionTime":378,"lastSuccessfullyExecutedCode":"# Predefined list to use as index\nnew_index = [['California', 'California', 'New York', 'Ohio'], \n             ['Los Angeles', 'San Francisco', 'New York', 'Cleveland']]\n\n# Create a multi-level index using predefined new_index\nchurn_new = pd.MultiIndex.from_arrays(new_index, names=[\"state\", \"city\"])\n\n# Print churn_new\nprint(churn_new)"},"cell_type":"code","id":"6c8e99ea-508f-4e5e-9668-7cb648b8a017","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"MultiIndex([('California',   'Los Angeles'),\n            ('California', 'San Francisco'),\n            (  'New York',      'New York'),\n            (      'Ohio',     'Cleveland')],\n           names=['state', 'city'])\n"}]},{"source":"# Create a multi-level index using predefined new_index\nchurn_new = pd.MultiIndex.from_arrays(new_index, names=['state', 'city'])\n\n# Assign the new index to the churn index\nchurn.index = churn_new\n\n# Reshape by stacking churn DataFrame\nchurn_stack = churn.stack()\n\n# Print churn_stack\nprint(churn_stack)","metadata":{"executionTime":203,"lastSuccessfullyExecutedCode":"# Create a multi-level index using predefined new_index\nchurn_new = pd.MultiIndex.from_arrays(new_index, names=['state', 'city'])\n\n# Assign the new index to the churn index\nchurn.index = churn_new\n\n# Reshape by stacking churn DataFrame\nchurn_stack = churn.stack()\n\n# Print churn_stack\nprint(churn_stack)"},"cell_type":"code","id":"6cd6d8fb-59cd-4d7f-9ec6-f608e6be07e9","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"state       city                            \nCalifornia  Los Angeles    Area code            408\n                           total_day_calls      116\n                           total_day_minutes    204\n            San Francisco  Area code            408\n                           total_day_calls      109\n                           total_day_minutes    287\nNew York    New York       Area code            415\n                           total_day_calls       84\n                           total_day_minutes     84\nOhio        Cleveland      Area code            510\n                           total_day_calls       67\n                           total_day_minutes     50\ndtype: int64\n"}]},{"source":"You are making progress in your customer's project. Now, you need to analyze a new dataset to find differences in the messages and gigabytes (GB) of data the customers use during the daytime and nighttime.\n\nTo that aim, you will reshape your dataset churn using different levels. The advantage of your new dataset is that the column indices have names. ","metadata":{},"cell_type":"markdown","id":"90de86c2-d070-4070-b5c6-155de8c47cf6"},{"source":"import pandas as pd\nchurn=pd.read_excel(\"fifa_players.xlsx\",\"Sheet11\")","metadata":{"executionTime":151,"lastSuccessfullyExecutedCode":"import pandas as pd\nchurn=pd.read_excel(\"fifa_players.xlsx\",\"Sheet11\")"},"cell_type":"code","id":"1ad3fc96-bd4e-475d-8e36-b636417e3416","execution_count":6,"outputs":[]},{"source":"print(churn)","metadata":{"executionTime":176,"lastSuccessfullyExecutedCode":"print(churn)"},"cell_type":"code","id":"c5403675-07fe-475c-bd79-381580f71e32","execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":"     time        feature       state           city  value\n0     day  text messages  California    Los Angeles     20\n1     day  text messages  California  San Francisco     40\n2     day  text messages    New York       New York     50\n3     day  text messages        Ohio      Cleveland    100\n4     day       total GB  California    Los Angeles      5\n5     day       total GB  California  San Francisco      5\n6     day       total GB    New York       New York      2\n7     day       total GB        Ohio      Cleveland      3\n8   night  text messages  California    Los Angeles     30\n9   night  text messages  California  San Francisco    100\n10  night  text messages    New York       New York     20\n11  night  text messages        Ohio      Cleveland     40\n12  night       total GB  California    Los Angeles     10\n13  night       total GB  California  San Francisco      5\n14  night       total GB    New York       New York      9\n15  night       total GB        Ohio      Cleveland      6\n"}]},{"source":"churn1=churn.pivot_table(index=[\"state\",\"city\"],columns=[\"time\",\"feature\"])\nprint(churn1)","metadata":{"executionTime":351,"lastSuccessfullyExecutedCode":"churn1=churn.pivot_table(index=[\"state\",\"city\"],columns=[\"time\",\"feature\"])\nprint(churn1)"},"cell_type":"code","id":"ed189617-ee47-4908-a865-901165687ad3","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"                                 value                                \ntime                               day                  night         \nfeature                  text messages total GB text messages total GB\nstate      city                                                       \nCalifornia Los Angeles              20        5            30       10\n           San Francisco            40        5           100        5\nNew York   New York                 50        2            20        9\nOhio       Cleveland               100        3            40        6\n"}]},{"source":"# Stack churn by the time column level\nchurn_time = churn1.stack(level=\"time\")\n\n# Print churn_time\nprint(churn_time)","metadata":{"executionTime":208,"lastSuccessfullyExecutedCode":"# Stack churn by the time column level\nchurn_time = churn1.stack(level=\"time\")\n\n# Print churn_time\nprint(churn_time)"},"cell_type":"code","id":"07cfefdc-7e9f-4ce5-badf-8d94a11dcf70","execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":"                                       value         \nfeature                        text messages total GB\nstate      city          time                        \nCalifornia Los Angeles   day              20        5\n                         night            30       10\n           San Francisco day              40        5\n                         night           100        5\nNew York   New York      day              50        2\n                         night            20        9\nOhio       Cleveland     day             100        3\n                         night            40        6\n"}]},{"source":"# Stack churn by the feature column level\nchurn_feature = churn1.stack(level=2)\n\n# Print churn_feature\nprint(churn_feature)","metadata":{"executionTime":200,"lastSuccessfullyExecutedCode":"# Stack churn by the feature column level\nchurn_feature = churn1.stack(level=2)\n\n# Print churn_feature\nprint(churn_feature)"},"cell_type":"code","id":"dc044193-1d6f-463f-b88b-ca770b82a237","execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":"                                       value      \ntime                                     day night\nstate      city          feature                  \nCalifornia Los Angeles   text messages    20    30\n                         total GB          5    10\n           San Francisco text messages    40   100\n                         total GB          5     5\nNew York   New York      text messages    50    20\n                         total GB          2     9\nOhio       Cleveland     text messages   100    40\n                         total GB          3     6\n"}]},{"source":"## Unstacking DataFrames\nCan we reshape a stacked DataFrame back into unstacked form?  \nPandas provides with the df.unstack() method which performs the inverse operation of stacking.  \nThen unstacking means rearranging the innermost row index to become the innermost column index.  \nWe can set the level= argument. If we don't specify it unstack() moves the last column level by default.   ","metadata":{},"cell_type":"markdown","id":"28e22118-5301-453b-b0de-810d15d4f271"},{"source":"International caller\n\nYou have a new task. You will analyze the pattern of customers on international and domestic calls.\n\nYou explore the churn dataset, which contains a multi-level row index. Again, you will reshape the data, as you expect it will help you to do further analysis.\n\nThe DataFrame churn is available for you. It contains data about minutes, calls, and charge for different times of the day, types of calls, and exited status.","metadata":{},"cell_type":"markdown","id":"bceaa4ec-8b29-45ae-929b-6cde38a0f9cd"},{"source":"import pandas as pd\nchurn=pd.read_excel(\"fifa_players.xlsx\",\"Sheet12\")","metadata":{"executionTime":211,"lastSuccessfullyExecutedCode":"import pandas as pd\nchurn=pd.read_excel(\"fifa_players.xlsx\",\"Sheet12\")"},"cell_type":"code","id":"2e64deb1-c2c6-459d-92bd-8b9bc71b3964","execution_count":11,"outputs":[]},{"source":"churn2=churn.pivot_table(index=['time','type','exited'])\nprint(churn2)","metadata":{"executionTime":218,"lastSuccessfullyExecutedCode":"churn2=churn.pivot_table(index=['time','type','exited'])\nprint(churn2)"},"cell_type":"code","id":"4646c4fa-9865-4fbb-b161-8bbf62504a71","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"                              calls  charge  minutes\ntime  type          exited                          \nday   International churn        97   31.37    184.5\n      National      churn       137   21.95    129.1\neve   International no churn    117   20.28    119.3\n      National      no churn     88   23.31    137.1\nnight International churn        67   56.59    332.9\n      National      no churn    103   18.77    110.4\n"}]},{"source":"# Reshape the churn2 DataFrame by unstacking\nchurn_unstack = churn.unstack()\n\n# Print churn_unstack\nprint(churn_unstack)","metadata":{"executionTime":269,"lastSuccessfullyExecutedCode":"# Reshape the churn2 DataFrame by unstacking\nchurn_unstack = churn.unstack()\n\n# Print churn_unstack\nprint(churn_unstack)"},"cell_type":"code","id":"587265e1-cbbd-4a60-87a3-80bfb79e70bb","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"time     0              day\n         1              day\n         2            night\n         3            night\n         4              eve\n         5              eve\ntype     0    International\n         1         National\n         2    International\n         3         National\n         4    International\n         5         National\nexited   0            churn\n         1            churn\n         2            churn\n         3         no churn\n         4         no churn\n         5         no churn\nminutes  0            184.5\n         1            129.1\n         2            332.9\n         3            110.4\n         4            119.3\n         5            137.1\ncalls    0               97\n         1              137\n         2               67\n         3              103\n         4              117\n         5               88\ncharge   0            31.37\n         1            21.95\n         2            56.59\n         3            18.77\n         4            20.28\n         5            23.31\ndtype: object\n"}]},{"source":"# Reshape churn2 by unstacking the first row level\nchurn_first = churn2.unstack(level=0)\n\n# Print churn_zero\nprint(churn_first)","metadata":{"executionTime":224,"lastSuccessfullyExecutedCode":"# Reshape churn2 by unstacking the first row level\nchurn_first = churn2.unstack(level=0)\n\n# Print churn_zero\nprint(churn_first)"},"cell_type":"code","id":"6c5aca33-cd86-4132-81d2-12fe804ed5af","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"                        calls               charge  ...        minutes              \ntime                      day    eve  night    day  ...  night     day    eve  night\ntype          exited                                ...                             \nInternational churn      97.0    NaN   67.0  31.37  ...  56.59   184.5    NaN  332.9\n              no churn    NaN  117.0    NaN    NaN  ...    NaN     NaN  119.3    NaN\nNational      churn     137.0    NaN    NaN  21.95  ...    NaN   129.1    NaN    NaN\n              no churn    NaN   88.0  103.0    NaN  ...  18.77     NaN  137.1  110.4\n\n[4 rows x 9 columns]\n"}]},{"source":"# Reshape churn2 by unstacking the second row level\nchurn_second = churn2.unstack(level=1)\n\n# Print churn_second\nprint(churn_second)","metadata":{"executionTime":309,"lastSuccessfullyExecutedCode":"# Reshape churn2 by unstacking the second row level\nchurn_second = churn2.unstack(level=1)\n\n# Print churn_second\nprint(churn_second)"},"cell_type":"code","id":"fdfc59e7-990c-4657-a6fa-37635f7dce3f","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":"                       calls           ...       minutes         \ntype           International National  ... International National\ntime  exited                           ...                       \nday   churn             97.0    137.0  ...         184.5    129.1\neve   no churn         117.0     88.0  ...         119.3    137.1\nnight churn             67.0      NaN  ...         332.9      NaN\n      no churn           NaN    103.0  ...           NaN    110.4\n\n[4 rows x 6 columns]\n"}]},{"source":"You discover some patterns when you reshaped the DataFrame. Now, you want to unstack the DataFrame again. This time you will choose which level to unstack and reorganize your indices. ","metadata":{},"cell_type":"markdown","id":"9d7e7c23-66cd-406e-a6e0-f02e86da4e41"},{"source":"# Sort the index in descending order\nchurn_time = churn2.unstack(level='time').sort_index(ascending=False)\n\n# Print churn_time\nprint(churn_time)","metadata":{"executionTime":276,"lastSuccessfullyExecutedCode":"# Sort the index in descending order\nchurn_time = churn2.unstack(level='time').sort_index(ascending=False)\n\n# Print churn_time\nprint(churn_time)"},"cell_type":"code","id":"3e6b5d5d-9ab6-44b2-b441-843c55f17632","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":"                        calls               charge  ...        minutes              \ntime                      day    eve  night    day  ...  night     day    eve  night\ntype          exited                                ...                             \nNational      no churn    NaN   88.0  103.0    NaN  ...  18.77     NaN  137.1  110.4\n              churn     137.0    NaN    NaN  21.95  ...    NaN   129.1    NaN    NaN\nInternational no churn    NaN  117.0    NaN    NaN  ...    NaN     NaN  119.3    NaN\n              churn      97.0    NaN   67.0  31.37  ...  56.59   184.5    NaN  332.9\n\n[4 rows x 9 columns]\n"}]},{"source":"Organizing your voicemail\n\nYou will perform one final task before moving to a new project. You will reshape the DataFrame churn again. This time, you'll reorganize a row index as a column index. After that, you will move a column index to a row index. To do this, you will first unstack the DataFrame, and then stack it.","metadata":{},"cell_type":"markdown","id":"fffa760a-310f-48c2-94bd-13cc1cbccd44"},{"source":"# Unstack churn by type level\nchurn_type = churn2.unstack(level=\"type\")\n\n# Stack the resulting DataFrame using the first column level\nchurn_final = churn_type.stack(level=0)\n\n# Print churn_type\nprint(churn_final)","metadata":{"executionTime":340,"lastSuccessfullyExecutedCode":"# Unstack churn by type level\nchurn_type = churn2.unstack(level=\"type\")\n\n# Stack the resulting DataFrame using the first column level\nchurn_final = churn_type.stack(level=0)\n\n# Print churn_type\nprint(churn_final)"},"cell_type":"code","id":"d138c907-d147-45c3-af4f-11f56f5988cf","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":"type                    International  National\ntime  exited                                   \nday   churn    calls            97.00    137.00\n               charge           31.37     21.95\n               minutes         184.50    129.10\neve   no churn calls           117.00     88.00\n               charge           20.28     23.31\n               minutes         119.30    137.10\nnight churn    calls            67.00       NaN\n               charge           56.59       NaN\n               minutes         332.90       NaN\n      no churn calls              NaN    103.00\n               charge             NaN     18.77\n               minutes            NaN    110.40\n"}]},{"source":"## Working with multiple levels  \nStack and unstack multiple levels at the same time.  \n\nThe swaplevel() method can swithc the order of two levels within the same axis.  \nThis means that we can swap the order of two row levels or two column levels. It has the axis= argument.  \n\nUnstacking several levels at the same time we just pass a list of the index numbers to the level argument. We can also use the level names.","metadata":{},"cell_type":"markdown","id":"5c62c8dd-1b1c-49d0-b65d-faafb31a75fc"},{"source":"Swap your SIM card\nNow it's time to go a step further and analyze the data to discover if a customer's cell phone plan is related to the customer leaving.\n\nYou explore the churn dataset and notice that the row levels are not well organized. First, you want to rearrange your row indicesso it's easier to reshape your DataFrame. ","metadata":{},"cell_type":"markdown","id":"a4ae288a-e267-4ca1-88a0-baed3b31eb54"},{"source":"import pandas as pd\nchurn=pd.read_excel(\"fifa_players.xlsx\",\"Sheet13\")","metadata":{"executionTime":265,"lastSuccessfullyExecutedCode":"import pandas as pd\nchurn=pd.read_excel(\"fifa_players.xlsx\",\"Sheet13\")"},"cell_type":"code","id":"b70c1557-8e39-44e7-a7e4-9a7e37d213bb","execution_count":1,"outputs":[]},{"source":"print(churn)","metadata":{"executionTime":275,"lastSuccessfullyExecutedCode":"print(churn)"},"cell_type":"code","id":"d2de6ab6-15c4-41d1-a1ee-f697509a7567","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"    year    exited       state         city       plan  value\n0   2019     churn  California  Los Angeles    minutes      0\n1   2019     churn  California  Los Angeles  voicemail      1\n2   2019     churn  California  Los Angeles       data      2\n3   2020     churn  California  Los Angeles    minutes      1\n4   2020     churn  California  Los Angeles  voicemail      1\n5   2020     churn  California  Los Angeles       data      5\n6   2019  no_churn  California  Los Angeles    minutes      0\n7   2019  no_churn  California  Los Angeles  voicemail      1\n8   2019  no_churn  California  Los Angeles       data      3\n9   2020  no_churn  California  Los Angeles    minutes      1\n10  2020  no_churn  California  Los Angeles  voicemail      0\n11  2020  no_churn  California  Los Angeles       data      2\n12  2019     churn    New York     New York    minutes      1\n13  2019     churn    New York     New York  voicemail      0\n14  2019     churn    New York     New York       data      5\n15  2020     churn    New York     New York    minutes      0\n16  2020     churn    New York     New York  voicemail      1\n17  2020     churn    New York     New York       data      2\n18  2019  no_churn    New York     New York    minutes      1\n19  2019  no_churn    New York     New York  voicemail      0\n20  2019  no_churn    New York     New York       data      4\n21  2020  no_churn    New York     New York    minutes      1\n22  2020  no_churn    New York     New York  voicemail      0\n23  2020  no_churn    New York     New York       data      6\n"}]},{"source":"churn1=churn.pivot_table(index=[\"exited\",\"state\",\"city\"],columns=[\"year\",\"plan\"])","metadata":{"executionTime":245,"lastSuccessfullyExecutedCode":"churn1=churn.pivot_table(index=[\"exited\",\"state\",\"city\"],columns=[\"year\",\"plan\"])"},"cell_type":"code","id":"c330bca5-c0d6-4c43-9181-3e3e7b35e2a9","execution_count":3,"outputs":[]},{"source":"print(churn1)","metadata":{"executionTime":328,"lastSuccessfullyExecutedCode":"print(churn1)"},"cell_type":"code","id":"211606f1-a465-4a6e-866f-e80cd685afbe","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"                                value                                         \nyear                             2019                   2020                  \nplan                             data minutes voicemail data minutes voicemail\nexited   state      city                                                      \nchurn    California Los Angeles     2       0         1    5       1         1\n         New York   New York        5       1         0    2       0         1\nno_churn California Los Angeles     3       0         1    2       1         0\n         New York   New York        4       1         0    6       1         0\n"}]},{"source":"# Switch the first and third row index levels in churn\nchurn_swap = churn1.swaplevel(0, 2)\n\nprint(churn_swap)\n\n# Reshape by unstacking the last row level \nchurn_unstack = churn_swap.unstack()\n\n# Print churn_unstack\nprint(churn_unstack)","metadata":{"executionTime":267,"lastSuccessfullyExecutedCode":"# Switch the first and third row index levels in churn\nchurn_swap = churn1.swaplevel(0, 2)\n\nprint(churn_swap)\n\n# Reshape by unstacking the last row level \nchurn_unstack = churn_swap.unstack()\n\n# Print churn_unstack\nprint(churn_unstack)"},"cell_type":"code","id":"ed73d2b2-90a1-4d6b-8f47-3efd45c7a8db","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"                                value                                         \nyear                             2019                   2020                  \nplan                             data minutes voicemail data minutes voicemail\ncity        state      exited                                                 \nLos Angeles California churn        2       0         1    5       1         1\nNew York    New York   churn        5       1         0    2       0         1\nLos Angeles California no_churn     3       0         1    2       1         0\nNew York    New York   no_churn     4       1         0    6       1         0\n                       value                   ...                            \nyear                    2019                   ...     2020                   \nplan                    data          minutes  ...  minutes voicemail         \nexited                 churn no_churn   churn  ... no_churn     churn no_churn\ncity        state                              ...                            \nLos Angeles California     2        3       0  ...        1         1        0\nNew York    New York       5        4       1  ...        1         1        0\n\n[2 rows x 12 columns]\n"}]},{"source":"# Unstack the first and second row level of churn\nchurn_unstack = churn1.unstack(level=[0, 1])\nprint(churn_unstack)","metadata":{"executionTime":228,"lastSuccessfullyExecutedCode":"# Unstack the first and second row level of churn\nchurn_unstack = churn1.unstack(level=[0, 1])\nprint(churn_unstack)"},"cell_type":"code","id":"d4c5cb54-f649-4b6f-8bdf-0b6e6a62706b","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"                 value                      ...                              \nyear              2019                      ...      2020                    \nplan              data                      ... voicemail                    \nexited           churn            no_churn  ...     churn   no_churn         \nstate       California New York California  ...  New York California New York\ncity                                        ...                              \nLos Angeles        2.0      NaN        3.0  ...       NaN        0.0      NaN\nNew York           NaN      5.0        NaN  ...       1.0        NaN      0.0\n\n[2 rows x 24 columns]\n"}]},{"source":"# Unstack the first and second row level of churn\nchurn_unstack = churn1.unstack(level=[0, 1])\n\n# Stack the resulting DataFrame using plan and year\nchurn_py = churn_unstack.stack(['plan', 'year'])\n\n# Switch the first and second column levels\nchurn_switch = churn_py.swaplevel(0,1,axis=1)\n\n# Print churn_switch\nprint(churn_switch)","metadata":{"executionTime":230,"lastSuccessfullyExecutedCode":"# Unstack the first and second row level of churn\nchurn_unstack = churn1.unstack(level=[0, 1])\n\n# Stack the resulting DataFrame using plan and year\nchurn_py = churn_unstack.stack(['plan', 'year'])\n\n# Switch the first and second column levels\nchurn_switch = churn_py.swaplevel(0,1,axis=1)\n\n# Print churn_switch\nprint(churn_switch)"},"cell_type":"code","id":"3cc24bfd-0026-4456-ad9c-d411ef386dd4","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"exited                          churn            no_churn         \n                                value               value         \nstate                      California New York California New York\ncity        plan      year                                        \nLos Angeles data      2019        2.0      NaN        3.0      NaN\n                      2020        5.0      NaN        2.0      NaN\n            minutes   2019        0.0      NaN        0.0      NaN\n                      2020        1.0      NaN        1.0      NaN\n            voicemail 2019        1.0      NaN        1.0      NaN\n                      2020        1.0      NaN        0.0      NaN\nNew York    data      2019        NaN      5.0        NaN      4.0\n                      2020        NaN      2.0        NaN      6.0\n            minutes   2019        NaN      1.0        NaN      1.0\n                      2020        NaN      0.0        NaN      1.0\n            voicemail 2019        NaN      0.0        NaN      0.0\n                      2020        NaN      1.0        NaN      0.0\n"}]},{"source":"## Handling missing data\n* Stack all column levels or unstack one row index level or choosing which levels stack or unstack.  \n\nThose operations can lead to nan values. Fortunatly, the unstack method has the argument fill_value=, which allows us to fill those values with any value.  \nIn the case of the stack method, we use the argument dropna=True/False and put .fillna(0), i.e.\ndf.stack(dropna=False).fillna(0)","metadata":{},"cell_type":"markdown","id":"5f4e8973-9f76-4260-91ab-6c568946e2b9"},{"source":"#Let us import the dataset and then pivot in the way is presented in the exercise\nimport pandas as pd\nchurn=pd.read_excel(\"fifa_players.xlsx\",\"Sheet14\")\nprint(churn)","metadata":{"executionTime":303,"lastSuccessfullyExecutedCode":"#Let us import the dataset and then pivot in the way is presented in the exercise\nimport pandas as pd\nchurn=pd.read_excel(\"fifa_players.xlsx\",\"Sheet14\")\nprint(churn)"},"cell_type":"code","id":"a4f13c89-56b8-426e-b28e-6d85737c6fe4","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"    total_day_calls  churn  ...  international_plan voice_mail_plan\n0           100.000  False  ...                  No             Yes\n1           115.000  False  ...                  No             Yes\n2            71.000  False  ...                 Yes             Yes\n3           120.000  False  ...                 Yes             Yes\n4           106.818  False  ...                  No              No\n5            90.900  False  ...                  No              No\n6            78.000  False  ...                 Yes              No\n7           109.000  False  ...                 Yes              No\n8             0.000   True  ...                  No             Yes\n9             0.000   True  ...                  No             Yes\n10            0.000   True  ...                 Yes             Yes\n11            0.000   True  ...                 Yes             Yes\n12          100.000   True  ...                  No              No\n13           95.000   True  ...                  No              No\n14           69.000   True  ...                 Yes              No\n15           87.000   True  ...                 Yes              No\n\n[16 rows x 6 columns]\n"}]},{"source":"churn1=churn.pivot_table(index=[\"state\",\"international_plan\",\"voice_mail_plan\"],columns=[\"churn\"])\nprint(churn1)","metadata":{"executionTime":359,"lastSuccessfullyExecutedCode":"churn1=churn.pivot_table(index=[\"state\",\"international_plan\",\"voice_mail_plan\"],columns=[\"churn\"])\nprint(churn1)"},"cell_type":"code","id":"1e4165fa-e03f-4ac9-b3b5-704ca3f7fabf","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":"                                         total_day_calls  ... total_night_calls\nchurn                                              False  ...              True\nstate international_plan voice_mail_plan                  ...                  \nLA    No                 No                      106.818  ...             119.0\n                         Yes                     100.000  ...               0.0\n      Yes                No                       78.000  ...             104.0\n                         Yes                      71.000  ...               0.0\nNY    No                 No                       90.900  ...             101.5\n                         Yes                     115.000  ...               0.0\n      Yes                No                      109.000  ...             113.0\n                         Yes                     120.000  ...               0.0\n\n[8 rows x 4 columns]\n"}]},{"source":"# Unstack churn level and fill missing values with zero\nchurn2 = churn1.stack(level=\"churn\")\nprint(churn2)","metadata":{"executionTime":293,"lastSuccessfullyExecutedCode":"# Unstack churn level and fill missing values with zero\nchurn2 = churn1.stack(level=\"churn\")\nprint(churn2)"},"cell_type":"code","id":"9c551b50-9b43-4577-a4b0-088e675f0ebc","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":"                                                total_day_calls  total_night_calls\nstate international_plan voice_mail_plan churn                                    \nLA    No                 No              False          106.818             96.909\n                                         True           100.000            119.000\n                         Yes             False          100.000             84.250\n                                         True             0.000              0.000\n      Yes                No              False           78.000             90.000\n                                         True            69.000            104.000\n                         Yes             False           71.000            101.000\n                                         True             0.000              0.000\nNY    No                 No              False           90.900            100.800\n                                         True            95.000            101.500\n                         Yes             False          115.000            121.000\n                                         True             0.000              0.000\n      Yes                No              False          109.000             99.000\n                                         True            87.000            113.000\n                         Yes             False          120.000             78.000\n                                         True             0.000              0.000\n"}]},{"source":"It's almost time to go home, but first, you need to finish your last task. You have a small dataset containing the total number of calls made by customers.\n\nTo perform your analysis, you need to reshape your churn data by stacking different levels. You know this process will generate missing data. You want to check if it is worth keeping the rows that contain all missing values, or if it's better to drop that information. ","metadata":{},"cell_type":"markdown","id":"ecdb7cdc-4401-47bd-8f0c-47ea8f9da202"},{"source":"import pandas as pd\nchurn=pd.read_excel(\"fifa_players.xlsx\",\"Sheet15\")\nprint(churn)","metadata":{"executionTime":295,"lastSuccessfullyExecutedCode":"import pandas as pd\nchurn=pd.read_excel(\"fifa_players.xlsx\",\"Sheet15\")\nprint(churn)"},"cell_type":"code","id":"13baaa6d-edb2-42f7-a014-fb835e9b5d26","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"                type          scope    LA  NY  CA\n0    total_day_calls  international  23.0   8   8\n1  total_night_calls  international  30.0  34  34\n2  total_night_calls       national   NaN  24  24\n"}]},{"source":"churn1=churn.pivot_table(columns=[\"type\",\"scope\"])\nprint(churn1)","metadata":{"executionTime":250,"lastSuccessfullyExecutedCode":"churn1=churn.pivot_table(columns=[\"type\",\"scope\"])\nprint(churn1)"},"cell_type":"code","id":"12ebf76b-7975-4762-bef9-20b54e39381c","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"type  total_day_calls total_night_calls         \nscope   international     international national\nCA                8.0              34.0     24.0\nLA               23.0              30.0      NaN\nNY                8.0              34.0     24.0\n"}]},{"source":"# Stack the level type from churn\nchurn_stack = churn1.stack(level=\"type\")\n\n# Fill the resulting missing values with zero \nchurn_fill = churn_stack.fillna(0)\n\n# Print churn_fill\nprint(churn_fill)","metadata":{"executionTime":344,"lastSuccessfullyExecutedCode":"# Stack the level type from churn\nchurn_stack = churn1.stack(level=\"type\")\n\n# Fill the resulting missing values with zero \nchurn_fill = churn_stack.fillna(0)\n\n# Print churn_fill\nprint(churn_fill)"},"cell_type":"code","id":"46586b93-4d03-4de8-9652-b6058f9e9cfe","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"scope                 international  national\n   type                                      \nCA total_day_calls              8.0       0.0\n   total_night_calls           34.0      24.0\nLA total_day_calls             23.0       0.0\n   total_night_calls           30.0       0.0\nNY total_day_calls              8.0       0.0\n   total_night_calls           34.0      24.0\n"}]},{"source":"## Reshaping and combinig data\nStatistical functions: .sum(), .mean(), .median(), .diff()  ","metadata":{},"cell_type":"markdown","id":"b05af804-1772-447c-81b0-5545fa81375c"},{"source":"import pandas as pd\nobesity=pd.read_excel(\"fifa_players.xlsx\",\"Sheet16\")\nprint(obesity)","metadata":{"executionTime":450,"lastSuccessfullyExecutedCode":"import pandas as pd\nobesity=pd.read_excel(\"fifa_players.xlsx\",\"Sheet16\")\nprint(obesity)"},"cell_type":"code","id":"1c26b043-39f7-4bff-b2c7-eeb277aabd0c","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"      country biological_sex  year  perc_obesity\n0   Argentina           Male  2005          21.5\n1   Argentina         Female  2005          24.2\n2   Argentina           Male  2015          26.8\n3   Argentina         Female  2015          28.5\n4       Japan           Male  2005           2.5\n5       Japan         Female  2005           2.6\n6       Japan           Male  2015           4.6\n7       Japan         Female  2015           3.6\n8      Norway           Male  2005          17.6\n9      Norway         Female  2005          18.6\n10     Norway           Male  2015          23.0\n11     Norway         Female  2015          22.2\n"}]},{"source":"obesity=obesity.pivot_table(index=[\"country\",\"biological_sex\",\"year\"])\nprint(obesity)","metadata":{"executionTime":291,"lastSuccessfullyExecutedCode":"obesity=obesity.pivot_table(index=[\"country\",\"biological_sex\",\"year\"])\nprint(obesity)"},"cell_type":"code","id":"0f5ee476-dafe-46be-8202-c1674ab1d35c","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"                               perc_obesity\ncountry   biological_sex year              \nArgentina Female         2005          24.2\n                         2015          28.5\n          Male           2005          21.5\n                         2015          26.8\nJapan     Female         2005           2.6\n                         2015           3.6\n          Male           2005           2.5\n                         2015           4.6\nNorway    Female         2005          18.6\n                         2015          22.2\n          Male           2005          17.6\n                         2015          23.0\n"}]},{"source":"# Unstack the first level and calculate the mean of the columns\nobesity_general = obesity.unstack(level=0).mean(axis=1)\n\n# Print obesity_general\nprint(obesity_general)","metadata":{"executionTime":277,"lastSuccessfullyExecutedCode":"# Unstack the first level and calculate the mean of the columns\nobesity_general = obesity.unstack(level=0).mean(axis=1)\n\n# Print obesity_general\nprint(obesity_general)"},"cell_type":"code","id":"6d7e8088-425c-4c92-9c51-3fc06da174b2","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"biological_sex  year\nFemale          2005    15.133333\n                2015    18.100000\nMale            2005    13.866667\n                2015    18.133333\ndtype: float64\n"}]},{"source":"# Unstack the second level and calculate the mean of the columns\nobesity_mean = obesity.unstack(level=1).mean(axis=1)\n\n# Print obesity_mean\nprint(obesity_mean)","metadata":{"executionTime":334,"lastSuccessfullyExecutedCode":"# Unstack the second level and calculate the mean of the columns\nobesity_mean = obesity.unstack(level=1).mean(axis=1)\n\n# Print obesity_mean\nprint(obesity_mean)"},"cell_type":"code","id":"5faac11d-0738-40ec-b9a4-a2e43922476d","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":"country    year\nArgentina  2005    22.85\n           2015    27.65\nJapan      2005     2.55\n           2015     4.10\nNorway     2005    18.10\n           2015    22.60\ndtype: float64\n"}]},{"source":"# Unstack the third level and calculate the difference between columns\nobesity_variation = obesity.unstack(level=2).diff(axis=1)\n\n# Print obesity_variation\nprint(obesity_variation)","metadata":{"executionTime":255,"lastSuccessfullyExecutedCode":"# Unstack the third level and calculate the difference between columns\nobesity_variation = obesity.unstack(level=2).diff(axis=1)\n\n# Print obesity_variation\nprint(obesity_variation)"},"cell_type":"code","id":"2ed59c7b-8164-4544-a26f-6830e17e629a","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"                         perc_obesity     \nyear                             2005 2015\ncountry   biological_sex                  \nArgentina Female                  NaN  4.3\n          Male                    NaN  5.3\nJapan     Female                  NaN  1.0\n          Male                    NaN  2.1\nNorway    Female                  NaN  3.6\n          Male                    NaN  5.4\n"}]},{"source":"#Stack the first level, get sum, and unstack the second level  \n\nobesity_sum = obesity.stack(level=0).sum(axis=1).unstack(level=1)\n\n#Print obesity_max  \n\nprint(obesity_sum)","metadata":{},"cell_type":"markdown","id":"f855b003-a27d-4a55-8bd6-d018253a2b7d"},{"source":"#Stack country level, group by country and get the mean  \n\nobesity_mean = obesity.stack(level=\"country\").groupby(\"country\").mean()\n\n#Print obesity_mean  \n\nprint(obesity_mean)","metadata":{},"cell_type":"markdown","id":"e5854acf-de07-482c-91b6-24d14a4e23da"},{"source":"#Stack country level, group by country and get the median  \n\nobesity_median = obesity.stack(\"country\").groupby(\"country\").median()\n\n#Print obesity_median  \n\nprint(obesity_median)","metadata":{},"cell_type":"markdown","id":"efe932d3-9e75-47b5-8542-866d325e091d"},{"source":"## Transforming a list-like column  \nWe could have columns that contain values in a list. This kind of columns are called list like column. It's hard to work when it is in the same cell. The best approach is to transform each list like column into a separate row. \n\nPandas provide the .explode() method that can help us with this operation.  ","metadata":{},"cell_type":"markdown","id":"d3519a8c-e1ca-4c04-a813-e514e73a27d0"},{"source":"import pandas as pd\nobesity=pd.read_excel(\"fifa_players.xlsx\",\"Sheet17\")\nprint(obesity)\n\n#Unfortunately the column bounds has the type strings, then before processing\n#we need to convert it to a column with list of floats. We use the library ast","metadata":{"executionTime":236,"lastSuccessfullyExecutedCode":"import pandas as pd\nobesity=pd.read_excel(\"fifa_players.xlsx\",\"Sheet17\")\nprint(obesity)"},"cell_type":"code","id":"58abd67d-86c4-4808-8729-f407ee9f7493","execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":"     country  perc_obesity        bounds\n0  Argentina          21.5  [15.4, 31.5]\n1    Germany          22.3  [16.2, 32.4]\n2      Japan           2.5    [1.1, 3.5]\n3     Norway          23.0  [13.1, 33.0]\n"}]},{"source":"import ast\n# Crea una nueva columna con una lista de números flotantes usando la función map()\nobesity[\"lista\"] = obesity[\"bounds\"].map(lambda x: ast.literal_eval(x))\n\n# Imprime el dataframe\nobesity=obesity.drop(\"bounds\",axis=1).rename(columns={\"lista\":\"bounds\"})\nprint(obesity)","metadata":{"executionTime":340,"lastSuccessfullyExecutedCode":"import ast\n# Crea una nueva columna con una lista de números flotantes usando la función map()\nobesity[\"lista\"] = obesity[\"bounds\"].map(lambda x: ast.literal_eval(x))\n\n# Imprime el dataframe\nobesity=obesity.drop(\"bounds\",axis=1).rename(columns={\"lista\":\"bounds\"})\nprint(obesity)"},"cell_type":"code","id":"cc65e753-7552-424c-aeb9-c257258d2eba","execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":"     country  perc_obesity        bounds\n0  Argentina          21.5  [15.4, 31.5]\n1    Germany          22.3  [16.2, 32.4]\n2      Japan           2.5    [1.1, 3.5]\n3     Norway          23.0  [13.1, 33.0]\n"}]},{"source":"You will analyze the mean obesity percentage in different countries, but this time, the obesity DataFrame has a new column named bounds. It contains the minimum and maximum values you can find in different parts of the same country.\n\nYou notice that these values are given in a list, so you decide that you need to transform that column. You would like to have each element in a new row. ","metadata":{},"cell_type":"markdown","id":"081e8263-d061-42b3-b704-969d0dc08c72"},{"source":"# Explode the values of bounds to a separate row\nobesity_bounds = obesity['bounds'].explode()\n\nprint(obesity_bounds)","metadata":{"executionTime":300,"lastSuccessfullyExecutedCode":"# Explode the values of bounds to a separate row\nobesity_bounds = obesity['bounds'].explode()\n\nprint(obesity_bounds)"},"cell_type":"code","id":"5e0d453d-e2b4-495a-bf88-d590d87d9d4f","execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":"0    15.4\n0    31.5\n1    16.2\n1    32.4\n2     1.1\n2     3.5\n3    13.1\n3    33.0\nName: bounds, dtype: object\n"}]},{"source":"# Merge obesity_bounds with country and perc_obesity columns of obesity using the indexes\nobesity_final = obesity[[\"country\", \"perc_obesity\"]].merge(obesity_bounds, \n                                        right_index=True, \n                                        left_index=True)\n\n# Print obesity_final\nprint(obesity_final)","metadata":{"executionTime":215,"lastSuccessfullyExecutedCode":"# Merge obesity_bounds with country and perc_obesity columns of obesity using the indexes\nobesity_final = obesity[[\"country\", \"perc_obesity\"]].merge(obesity_bounds, \n                                        right_index=True, \n                                        left_index=True)\n\n# Print obesity_final\nprint(obesity_final)"},"cell_type":"code","id":"7460499c-91e4-419a-888b-af56f999c8db","execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":"     country  perc_obesity bounds\n0  Argentina          21.5   15.4\n0  Argentina          21.5   31.5\n1    Germany          22.3   16.2\n1    Germany          22.3   32.4\n2      Japan           2.5    1.1\n2      Japan           2.5    3.5\n3     Norway          23.0   13.1\n3     Norway          23.0   33.0\n"}]},{"source":"You were able to transform the list-like column successfully, but you are not satisfied with the steps you had to take. You want to find an easier way to get the same reshaped DataFrame.\n\nYou remembered what you learned about exploding list-like columns, and you will apply a new strategy. ","metadata":{},"cell_type":"markdown","id":"343a89ec-4942-49b4-a2f4-919effb7b017"},{"source":"# Transform the list-like column named bounds  \nobesity_explode = obesity.explode(\"bounds\")\n\n# Modify obesity_explode by resetting the index \nobesity_explode.reset_index(drop=True, inplace=True)\n\n# Print obesity_explode\nprint(obesity_explode)","metadata":{"executionTime":228,"lastSuccessfullyExecutedCode":"# Transform the list-like column named bounds  \nobesity_explode = obesity.explode(\"bounds\")\n\n# Modify obesity_explode by resetting the index \nobesity_explode.reset_index(drop=True, inplace=True)\n\n# Print obesity_explode\nprint(obesity_explode)"},"cell_type":"code","id":"07be8e2c-6763-485f-a949-d86433a72312","execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":"     country  perc_obesity bounds\n0  Argentina          21.5   15.4\n1  Argentina          21.5   31.5\n2    Germany          22.3   16.2\n3    Germany          22.3   32.4\n4      Japan           2.5    1.1\n5      Japan           2.5    3.5\n6     Norway          23.0   13.1\n7     Norway          23.0   33.0\n"}]},{"source":"The good old split\n\nYou have to do one last task for the obesity project. Your colleague gave you a new dataset to analyze with which you will perform the same analysis as before.\n\nAfter inspecting the dataset obesity, you realize that you have the same columns as before, but the bounds column is not a list. This time, the column contains two values separated with a hyphen in the form of string. ","metadata":{},"cell_type":"markdown","id":"024c4171-f1a5-4cca-84fb-ac7a9e6d8b66"},{"source":"#Split the columns bounds using a hyphen as delimiter  \nobesity_split = obesity[\"bounds\"].str.split(\"-\")\n\n#Print obesity_split  \nprint(obesity_split)","metadata":{},"cell_type":"markdown","id":"15811e75-65c8-4b6e-ade9-b2a54a47f84e"},{"source":"#Transform the column bounds in the obesity DataFrame  \nobesity_split = obesity.assign(bounds=obesity['bounds'].str.split('-')).explode(\"bounds\")\n\n#Print obesity_split  \nprint(obesity_split)","metadata":{},"cell_type":"markdown","id":"f50f3edf-4213-476e-95c6-685d8d1159a0"},{"source":"## Reading nested data into dataframe\nJSON format. JSON stands for JavaScript Object Notation. It is a data interchange format.  \nSome JSONs can be nested;like a dictionary within a dictionary.  \n\nWe use the json_normalize() function in pandas. It has the parameter sep=\"\".  \n\nIn the case of nested dictionarys inside the json document, we can use the record_path= parameter. It tells pandas what key path leads to each individual observation in JSON.  \n\nWe can add a third parameter, the meta= parameter. It tells pandas what data we want to include from the rest of the JSON.  ","metadata":{},"cell_type":"markdown","id":"e8aa5708-d99c-4ff7-841a-c8060d8ac252"},{"source":"You are curious about a movies dataset you've had on your computer for some time now that contains data about different movies. You would like to analyze that data, but you realize it's in a nested JSON format.\n\nTo read it into a DataFrame, you will need to use the function you have just learned. After that, you will reshape the resulting DataFrame to make it easier to work with. ","metadata":{},"cell_type":"markdown","id":"a1c248d0-ec96-47fa-baa1-57e3249c925d"},{"source":"import json\nimport pandas as pd","metadata":{"executionTime":436,"lastSuccessfullyExecutedCode":"import json\nimport pandas as pd"},"cell_type":"code","id":"f9f7b1ff-348e-4a42-9ce4-beab3d1cf843","execution_count":8,"outputs":[]},{"source":"# Load JSON: json_data\nwith open(\"movies.json\") as json_file:\n    movies=json.load(json_file)","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Load JSON: json_data\nwith open(\"movies.json\") as json_file:\n    movies=json.load(json_file)"},"cell_type":"code","id":"ff699486-6431-4749-9b98-b978815ce14a","execution_count":12,"outputs":[]},{"source":"print(movies)","metadata":{"executionTime":413,"lastSuccessfullyExecutedCode":"print(movies)"},"cell_type":"code","id":"da244786-107a-4a77-af8e-cf506d8bae7a","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"[{'director': 'Woody Allen', 'producer': 'Letty Aronson', 'features': {'title': 'Magic in the Moonlight', 'year': 2014}}, {'director': 'Niki Caro', 'producer': 'Jason Reed', 'features': {'title': 'Mulan', 'year': 2020}}]\n"}]},{"source":"from pandas import json_normalize\n# Normalize movies and separate the new columns with an underscore \nmovies_norm = json_normalize(movies,sep=\"_\")\n\n# Reshape using director and producer as index, create movies from column starting from features\nmovies_long = pd.wide_to_long(movies_norm, stubnames=\"features\", \n                      i=[\"director\",\"producer\"], j=\"movies\", \n                      sep=\"_\", suffix=\"\\w+\")\n\n# Print movies_long\nprint(movies_long)","metadata":{"executionTime":706,"lastSuccessfullyExecutedCode":"from pandas import json_normalize\n# Normalize movies and separate the new columns with an underscore \nmovies_norm = json_normalize(movies,sep=\"_\")\n\n# Reshape using director and producer as index, create movies from column starting from features\nmovies_long = pd.wide_to_long(movies_norm, stubnames=\"features\", \n                      i=[\"director\",\"producer\"], j=\"movies\", \n                      sep=\"_\", suffix=\"\\w+\")\n\n# Print movies_long\nprint(movies_long)"},"cell_type":"code","id":"ec000d06-0133-4e77-a121-f2c7f66f9105","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"                                                features\ndirector    producer      movies                        \nWoody Allen Letty Aronson title   Magic in the Moonlight\n                          year                      2014\nNiki Caro   Jason Reed    title                    Mulan\n                          year                      2020\n"}]},{"source":"#Load JSON data\nimport json\nimport pandas as pd\nwith open(\"movies2.json\") as json_file:\n    movies2=json.load(json_file)","metadata":{"executionTime":393,"lastSuccessfullyExecutedCode":"#Load JSON data\nimport json\nimport pandas as pd\nwith open(\"movies2.json\") as json_file:\n    movies2=json.load(json_file)"},"cell_type":"code","id":"c233a052-c917-4bfd-9a68-d8f641d45abf","execution_count":20,"outputs":[]},{"source":"print(movies2)","metadata":{"executionTime":352,"lastSuccessfullyExecutedCode":"print(movies2)"},"cell_type":"code","id":"a49f6acb-aaba-42a9-99fb-45df5aa463fb","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"[{'director': 'Woody Allen', 'producer': 'Letty Aronson', 'features': [{'title': 'Magic in the Moonlight', 'year': 2014}, {'title': 'Vicky Cristina Barcelona', 'year': 2008}, {'title': 'Midnight in Paris', 'year': 2011}]}, {'director': 'Niki Caro', 'producer': 'Jason Reed', 'features': [{'title': 'Mulan', 'year': 2020}]}]\n"}]},{"source":"# Normalize the JSON contained in movies\nnormalize_movies = json_normalize(movies2)\n\n# Print normalize_movies\nprint(normalize_movies)","metadata":{"executionTime":307,"lastSuccessfullyExecutedCode":"# Normalize the JSON contained in movies\nnormalize_movies = json_normalize(movies2)\n\n# Print normalize_movies\nprint(normalize_movies)"},"cell_type":"code","id":"847f9d34-c9cc-4ff4-ad74-4cec2a3432e1","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":"      director  ...                                           features\n0  Woody Allen  ...  [{'title': 'Magic in the Moonlight', 'year': 2...\n1    Niki Caro  ...                 [{'title': 'Mulan', 'year': 2020}]\n\n[2 rows x 3 columns]\n"}]},{"source":"# Specify the features column as the list of records \nnormalize_movies = json_normalize(movies2, \n                                  record_path=\"features\")\n\n# Print normalize_movies\nprint(normalize_movies)","metadata":{"executionTime":583,"lastSuccessfullyExecutedCode":"# Specify the features column as the list of records \nnormalize_movies = json_normalize(movies2, \n                                  record_path=\"features\")\n\n# Print normalize_movies\nprint(normalize_movies)"},"cell_type":"code","id":"e7fcae47-ec9d-4865-b0e5-e3cfd5c55f73","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":"                      title  year\n0    Magic in the Moonlight  2014\n1  Vicky Cristina Barcelona  2008\n2         Midnight in Paris  2011\n3                     Mulan  2020\n"}]},{"source":"# Specify director and producer to use as metadata for each record \nnormalize_movies = json_normalize(movies2, \n                                  record_path='features', \n                                  meta=[\"director\",\"producer\"])\n\n# Print normalize_movies\nprint(normalize_movies)","metadata":{"executionTime":306,"lastSuccessfullyExecutedCode":"# Specify director and producer to use as metadata for each record \nnormalize_movies = json_normalize(movies2, \n                                  record_path='features', \n                                  meta=[\"director\",\"producer\"])\n\n# Print normalize_movies\nprint(normalize_movies)"},"cell_type":"code","id":"cf8eca46-259a-4ac5-a34d-d9ed58206557","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":"                      title  year     director       producer\n0    Magic in the Moonlight  2014  Woody Allen  Letty Aronson\n1  Vicky Cristina Barcelona  2008  Woody Allen  Letty Aronson\n2         Midnight in Paris  2011  Woody Allen  Letty Aronson\n3                     Mulan  2020    Niki Caro     Jason Reed\n"}]},{"source":"## Dealing with nested data columns  \nSuppose we have  \nwriters=[\"\",\"\"]  \nbooks=[\"{:,:}\",\"{:,:}\"]  \n\nWe can convert it to a dataframe with  \ncollection=pd.DataFrame(dict(writers=writers,books=books)).  \n\nThe book columns will contain nested data.  \nIn order to handle that, let's import the json module. Now, we will load the JSON string contained in books into a dictionary.   \n\nimport json  \nbook=collection[\"books\"].apply(json.loads).apply(pd.Series)  \n\nIt creates one column per key. The we concatenate back  \n\ncollection=collection.drop(columns=\"books\")\npd.concat([collection,books],axis=1)  \n\nAnother approach  \n\nimport json  \nbooks=collection[\"books\"].apply(json.loads).to_list()  \nbooks_dump=json.dumps(books)  \nnew_books=pd.read_json(books_dump)  \n\nThen we can concat this dataframe  \npd.concat([collection[\"writers\"],new_books],axis=1)","metadata":{},"cell_type":"markdown","id":"99b2ebfe-1029-4910-b323-7a82695ad35e"},{"source":" A client has provided data about birds he wants to classify.\n\nYou examine the data and realize that it's in a bad format - the list of birds is in one file, and the characteristics of the birds are in another.\n\nYou manage to read the bird names into a list called names. You read the bird facts into another list called bird_facts, but this list contains dictionaries in string format.\n\nTo have a usable DataFrame, you will need to perform several operations.","metadata":{},"cell_type":"markdown","id":"5ac3ddff-91da-42a5-ac75-c6d2ec81ddc3"},{"source":"import pandas as pd\nimport json\nnames=['Killdeer', 'Chipping Sparrow', 'Cedar Waxwing']\nbird_facts=['{\"Size\" : \"Large\", \"Color\": \"Golden brown\", \"Behavior\": \"Runs swiftly along ground\", \"Habitat\": \"Rocky areas\"}','{\"Size\":\"Small\", \"Color\": \"Gray-white\", \"Behavior\": \"Often in flocks\", \"Habitat\": \"Open woodlands\"}','{\"Size\":\"Small\", \"Color\": \"Gray-brown\", \"Behavior\": \"Catch insects over open water\", \"Habitat\": \"Parks\"}']","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport json\nnames=['Killdeer', 'Chipping Sparrow', 'Cedar Waxwing']\nbird_facts=['{\"Size\" : \"Large\", \"Color\": \"Golden brown\", \"Behavior\": \"Runs swiftly along ground\", \"Habitat\": \"Rocky areas\"}','{\"Size\":\"Small\", \"Color\": \"Gray-white\", \"Behavior\": \"Often in flocks\", \"Habitat\": \"Open woodlands\"}','{\"Size\":\"Small\", \"Color\": \"Gray-brown\", \"Behavior\": \"Catch insects over open water\", \"Habitat\": \"Parks\"}']"},"cell_type":"code","id":"ddb4a56d-fa79-429e-ba98-5c4da2f5aeb2","execution_count":13,"outputs":[]},{"source":"# Define birds reading names and bird_facts lists into names and bird_facts columns \nbirds = pd.DataFrame(dict(names=names,bird_facts=bird_facts))\n\n# Print birds\nprint(birds)","metadata":{"executionTime":395,"lastSuccessfullyExecutedCode":"# Define birds reading names and bird_facts lists into names and bird_facts columns \nbirds = pd.DataFrame(dict(names=names,bird_facts=bird_facts))\n\n# Print birds\nprint(birds)"},"cell_type":"code","id":"cb443819-8efc-4a18-af91-20f7572611d3","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"              names                                         bird_facts\n0          Killdeer  {\"Size\" : \"Large\", \"Color\": \"Golden brown\", \"B...\n1  Chipping Sparrow  {\"Size\":\"Small\", \"Color\": \"Gray-white\", \"Behav...\n2     Cedar Waxwing  {\"Size\":\"Small\", \"Color\": \"Gray-brown\", \"Behav...\n"}]},{"source":"# Define birds reading names and bird_facts lists into names and bird_facts columns\nbirds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))\n\n# Apply to bird_facts column the function loads from json module\ndata_split = birds['bird_facts'].apply(json.loads).apply(pd.Series)\n\nprint(data_split)","metadata":{"executionTime":446,"lastSuccessfullyExecutedCode":"# Define birds reading names and bird_facts lists into names and bird_facts columns\nbirds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))\n\n# Apply to bird_facts column the function loads from json module\ndata_split = birds['bird_facts'].apply(json.loads).apply(pd.Series)\n\nprint(data_split)"},"cell_type":"code","id":"6ea3cee5-6846-4fb8-a9d2-a0b3f716a136","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":"    Size         Color                       Behavior         Habitat\n0  Large  Golden brown      Runs swiftly along ground     Rocky areas\n1  Small    Gray-white                Often in flocks  Open woodlands\n2  Small    Gray-brown  Catch insects over open water           Parks\n"}]},{"source":"# Define birds reading names and bird_facts lists into names and bird_facts columns\nbirds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))\n\n# Apply to bird_facts column the function loads from json module\ndata_split = birds['bird_facts'].apply(json.loads).apply(pd.Series)\n\n# Remove the bird_facts column from birds\nbirds = birds.drop(columns='bird_facts')\n\n# Concatenate the columns of birds and data_split\nbirds = pd.concat([birds,data_split], axis=1)\n\n# Print birds\nprint(birds)","metadata":{"executionTime":817,"lastSuccessfullyExecutedCode":"# Define birds reading names and bird_facts lists into names and bird_facts columns\nbirds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))\n\n# Apply to bird_facts column the function loads from json module\ndata_split = birds['bird_facts'].apply(json.loads).apply(pd.Series)\n\n# Remove the bird_facts column from birds\nbirds = birds.drop(columns='bird_facts')\n\n# Concatenate the columns of birds and data_split\nbirds = pd.concat([birds,data_split], axis=1)\n\n# Print birds\nprint(birds)"},"cell_type":"code","id":"80a5bf4f-ae38-441a-9762-0298ff68ae23","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":"              names   Size  ...                       Behavior         Habitat\n0          Killdeer  Large  ...      Runs swiftly along ground     Rocky areas\n1  Chipping Sparrow  Small  ...                Often in flocks  Open woodlands\n2     Cedar Waxwing  Small  ...  Catch insects over open water           Parks\n\n[3 rows x 5 columns]\n"}]},{"source":"You want to read the birds data into a DataFrame like you did in the previous exercise, but this time, you would like to try a different approach.\n\nYou would like to have a code that you can reuse in this situations, so you want to establish the fastest strategy to convert it into a usable DataFrame. You think that working with the json format could speed up the process.","metadata":{},"cell_type":"markdown","id":"116ea6eb-044b-4256-84a4-8394936f0367"},{"source":"# Define birds reading names and bird_facts lists into names and bird_facts columns\nbirds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))\n\n# Apply json.loads to the bird_facts column and transform it to a list \nbirds_facts = birds['bird_facts'].apply(json.loads).to_list()\n\n# Convert birds_fact into a JSON \nbirds_dump = json.dumps(birds_facts)\n\n# Read the JSON birds_dump into a DataFrame\nbirds_df = pd.read_json(birds_dump)\n\n# Concatenate the 'names' column of birds with birds_df \nbirds_final = pd.concat([birds[\"names\"], birds_df], axis=1)\n\n# Print birds_final\nprint(birds_final)","metadata":{"executionTime":576,"lastSuccessfullyExecutedCode":"# Define birds reading names and bird_facts lists into names and bird_facts columns\nbirds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))\n\n# Apply json.loads to the bird_facts column and transform it to a list \nbirds_facts = birds['bird_facts'].apply(json.loads).to_list()\n\n# Convert birds_fact into a JSON \nbirds_dump = json.dumps(birds_facts)\n\n# Read the JSON birds_dump into a DataFrame\nbirds_df = pd.read_json(birds_dump)\n\n# Concatenate the 'names' column of birds with birds_df \nbirds_final = pd.concat([birds[\"names\"], birds_df], axis=1)\n\n# Print birds_final\nprint(birds_final)"},"cell_type":"code","id":"0c7f19cc-3f62-4e2b-8f8c-dff01072c5b9","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":"              names   Size  ...                       Behavior         Habitat\n0          Killdeer  Large  ...      Runs swiftly along ground     Rocky areas\n1  Chipping Sparrow  Small  ...                Often in flocks  Open woodlands\n2     Cedar Waxwing  Small  ...  Catch insects over open water           Parks\n\n[3 rows x 5 columns]\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}